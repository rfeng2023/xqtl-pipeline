{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# TWAS, cTWAS and MR\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This module provides software implementations for transcriptome-wide association analysis (TWAS), Quantile TWAS and performs variant selection for providing sparse signals for cTWAS (causal TWAS) analysis as described in Qian et al (2024+) the multi-group cTWAS method. It will additionally perform Mendelian Randomization using fine-mapping instrumental variables (IV) as described in Zhang et al 2020 for \"causal\" effects estimation and model validation, with the unit of analysis being a single gene-trait pair.\n",
    "\n",
    "This procedure is a continuation of the SuSiE-TWAS workflow --- it assumes that xQTL fine-mapping has been performed and moleuclar traits prediction weights pre-computed (to be used for TWAS). Cross validation for TWAS weights is optional but highly recommended.\n",
    "\n",
    "Quantile TWAS extends traditional TWAS by testing genetic effects at different quantiles of the trait distribution, which provides insights into genetic associations that vary across the distribution rather than just at the mean.\n",
    "\n",
    "GWAS data required are GWAS summary statistics and LD matrix for the region of interest.\n",
    "\n",
    "### Step 1: TWAS \n",
    "\n",
    "1. Extract GWAS z-score for region of interest and corresponding LD matrix.\n",
    "* Note: When loading GWAS summary statistics, specifying a column_file_path (YAML format) enables the use of load_rss_data(). This function standardizes column names and can flexibly generate missing \"z\" or \"beta\" columns using the col_to_flip option. If no column_file_path is provided, the simpler tabix_region() is used instead.\n",
    "* Note: To handle comment lines in the summary statistics file, use the --comment_string option to specify the comment symbol (e.g., --comment_string \"#\", with quotation marks). By default, no comment symbol is assumed.\n",
    "2. (Optional) perform allele matching QC for the LD matrix with summary stats.\n",
    "3. Process weights: for a number of methods such as LASSO, Elastic Net and mr.ash we have to take the weights as is for QTL variants overlapping with GWAS variants. For SuSiE weights it can be adjusted to exactly match GWAS variants.\n",
    "4. Perofrm TWAS test for multiple sets of weights. \n",
    "5. For each gene, filter TWAS results by keeping the best model selected by CV. Drop the genes that don't show good evidence of TWAS prediction weights.\n",
    "\n",
    "### Step 2: Variant Selection for Imputable Genes via the Best Prediction Methods\n",
    "1. Determine if the gene is imputable at each context based on the twas_cv performance by adjusted $r^2$ (>=0.01) and p-values (<0.05).\n",
    "2. The imputable gene-context pair will go through variant selection step. Maximum 10 variants with top pip selected from either `top_loci` table or SuSiE CS set. \n",
    "3. Harmonize weights against LD reference and udpate SuSiE weight. \n",
    "4. Extract weights by best model for the context then by the variant names were selected from the previous step\n",
    "\n",
    "### Step 3: cTWAS analysis\n",
    "\n",
    "**FIXME: add more documentation here**\n",
    "\n",
    "### Step 4: MR for candidate genes\n",
    "\n",
    "1. Limit MR only to those showing some evidence of cTWAS significance AND have strong instrumental variable (fine-mapping PIP or CS). \n",
    "2. Use fine-mapped xQTL with GWAS data to perform MR. \n",
    "3. For multiple IV, aggregate individual IV estimates using a fixed-effect meta-analysis procedure.\n",
    "4. Identify and exclude results with severe violations of the exclusion restriction (ER) assumption.\n",
    "\n",
    "### Step 5: Quantile TWAS analysis\n",
    "- Use pre-computed TWAS weights (beta for now) for quantile-specific testing.\n",
    "- For each quantile level, cluster and integrate by fixed and dynamic region groups, and extract relevant GWAS z-scores and LD matrix for the region of interest.\n",
    "- Perform quantile region-specific association tests, identifying genetic variants with effects that vary across different quantile regions of the phenotype distribution.\n",
    "\n",
    "## Input\n",
    "\n",
    "### GWAS Data Input Interface (Similar to `susie_rss`)\n",
    "\n",
    "I. **GWAS Summary Statistics Files**\n",
    "- **Input**: Vector of files for one or more GWAS studies.\n",
    "- **Format**: \n",
    "  - Tab-delimited files that is tabix-indexed by the first `chrom`(or `#chrom`) and second `pos` column. \n",
    "  - First 4 columns: `chrom` or `#chrom`, `pos`, `A1`, `A2`\n",
    "  - Additional columns can be loaded using column mapping file see below  \n",
    "  - If MR, `effect_allele_frequency` and sample size columns(either `n_sample` or `n_case`&`n_ccontrol`) are required.\n",
    "- **Column Mapping files (optional)**:\n",
    "  - Optional YAML file for custom column mapping.\n",
    "  - Required columns: `chrom`, `pos`, `A1`, `A2`, `z` or (`betahat` and `sebetahat`).\n",
    "  - Optional columns: `n`, `var_y` (relevant to fine-mapping).\n",
    "\n",
    "II. **GWAS Summary Statistics Meta-File**: this is optional and helpful when there are lots of GWAS data to process via the same command\n",
    "- **Columns**: `study_id`, chromosome number, path to summary statistics file, optional path to column mapping file.\n",
    "- **Note**: Chromosome number `0` indicates a genome-wide file.\n",
    "\n",
    "eg: `gwas_meta.tsv`\n",
    "\n",
    "```\n",
    "study_id    chrom    file_path                 column_mapping_file\n",
    "study1      1        gwas1.tsv.gz         column_mapping.yml\n",
    "study1      2        gwas2.tsv.gz         column_mapping.yml\n",
    "study2      0        gwas3.tsv.gz         column_mapping.yml\n",
    "```\n",
    "\n",
    "If both summary stats file (I) and meta data file (II) are specified we will take the union of the two.\n",
    "\n",
    "\n",
    "III. **LD Reference Metadata File**\n",
    "- **Format**: Single TSV file.\n",
    "- **Contents**:\n",
    "  - Columns: `#chrom`, `start`, `end`, path to the LD matrix, genomic build.\n",
    "  - LD matrix path format: comma-separated, first entry is the LD matrix, second is the bim file.\n",
    "- **Documentation**: Refer to [our LD reference preparation document](https://statfungen.github.io/xqtl-protocol/code/reference_data/ld_reference_generation.html) for detailed information.\n",
    "\n",
    "### Output of Fine-Mapping & TWAS Pipeline\n",
    "\n",
    "**xQTL Weight Database Metadata File**: \n",
    "- **Essential columns**: `#chr`, `start`, `end`, `TSS`, `region_id`, `original_data`\n",
    "- **Structure of the weight database**: \n",
    "  - RDS format.\n",
    "  - Organized hierarchically: region → context → weight matrix.\n",
    "  - Each column represents a different method.\n",
    "\n",
    "eg: `xqtl_meta.tsv`\n",
    "\n",
    "```\n",
    "#chr start end region_id TSS original_data combined_data combined_data_sumstats contexts contexts_top_loci\n",
    "chr1 0 6480000 ENSG00000008128 1724356 \"KNIGHT_pQTL.ENSG00000008128.univariate_susie_twas_weights.rds, MiGA_eQTL.ENSG00000008128.univariate_susie_twas_weights.rds, MSBB_eQTL.ENSG00000008128.univariate_susie_twas_weights.rds, ROSMAP_Bennett_Klein_pQTL.ENSG00000008128.univariate_susie_twas_weights.rds, ROSMAP_DeJager_eQTL.ENSG00000008128.univariate_susie_twas_weights.rds, ROSMAP_Kellis_eQTL.ENSG00000008128.univariate_susie_twas_weights.rds, ROSMAP_mega_eQTL.ENSG00000008128.univariate_susie_twas_weights.rds, STARNET_eQTL.ENSG00000008128.univariate_susie_twas_weights.rds\" Fungen_xQTL.ENSG00000008128.cis_results_db.export.rds Fungen_xQTL.ENSG00000008128.cis_results_db.export_sumstats.rds Knight_eQTL_brain,MiGA_GFM_eQTL,MiGA_GTS_eQTL,MiGA_SVZ_eQTL,MiGA_THA_eQTL,BM_10_MSBB_eQTL,BM_22_MSBB_eQTL,BM_36_MSBB_eQTL,BM_44_MSBB_eQTL,monocyte_ROSMAP_eQTL,Mic_DeJager_eQTL,Ast_DeJager_eQTL,Oli_DeJager_eQTL,Exc_DeJager_eQTL,Inh_DeJager_eQTL,DLPFC_DeJager_eQTL,PCC_DeJager_eQTL,AC_DeJager_eQTL,Mic_Kellis_eQTL,Ast_Kellis_eQTL,Oli_Kellis_eQTL,OPC_Kellis_eQTL,Exc_Kellis_eQTL,Inh_Kellis_eQTL,Ast_mega_eQTL,Exc_mega_eQTL,Inh_mega_eQTL,Oli_mega_eQTL,STARNET_eQTL_Mac Knight_eQTL_brain,MiGA_GFM_eQTL,MiGA_GTS_eQTL,MiGA_SVZ_eQTL,MiGA_THA_eQTL,BM_10_MSBB_eQTL,BM_22_MSBB_eQTL,BM_36_MSBB_eQTL,BM_44_MSBB_eQTL,monocyte_ROSMAP_eQTL,Mic_DeJager_eQTL,Ast_DeJager_eQTL,Oli_DeJager_eQTL,Exc_DeJager_eQTL,Inh_DeJager_eQTL,DLPFC_DeJager_eQTL,PCC_DeJager_eQTL,AC_DeJager_eQTL,Mic_Kellis_eQTL,Ast_Kellis_eQTL,Oli_Kellis_eQTL,OPC_Kellis_eQTL,Exc_Kellis_eQTL,Inh_Kellis_eQTL,Ast_mega_eQTL,Exc_mega_eQTL,Inh_mega_eQTL,Oli_mega_eQTL,STARNET_eQTL_Mac\n",
    "```\n",
    "\n",
    "This file is automatically generated as part of the FunGen-xQTL protocol, although only the essential columns are relevant to our application here.\n",
    "\n",
    "\n",
    "### TWAS region information\n",
    "\n",
    "This is required for cTWAS analysis, where multiple TWAS and SNP data within each region are combined for joint inference to select the variables, either genes or SNPs, to figure out which variables are likely to be directly associated with the phenotype of interest, rather than being associated through correlations with true causal variables.\n",
    "\n",
    "```\n",
    "chrom    start    end    block_id  \n",
    "1        1000     5000   block1    \n",
    "2        2000     6000   block2\n",
    "3        3000     7000   block3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing: ~X min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "\n",
    "I. A table with the following contents\n",
    "\n",
    "```\n",
    "gwas_study, chrom, start, end, block, gene, TSS, context, is_imputable, method, is_selected_method, rsq_adj_cv, pval_cv, twas_z, twas_pval\n",
    "```\n",
    "\n",
    "where\n",
    "- `TSS`: Transcription start site\n",
    "- `start` and `end`: start and end position of the gene from the [extended TADB window](https://github.com/cumc/xqtl-analysis/blob/main/resource/TADB_enhanced_cis.coding.bed) for cis-finemapping\n",
    "- `is_imputable`: status for wether this gene-context pair has  cross-validated performance with r-square > 0.01 and pvalue < 0.05 in at least one method for expression level prediction.\n",
    "- `rsq_cv`: cross validation test of r-square for a method.\n",
    "- `pval_cv`: cross-validation test of predictive performance p-value for a method.\n",
    "- `is_selected_method`: status of being best performing method (for each gene–context pair), we pick the best model with highest cross validation r-square with cross validation pvalue < 0.05\n",
    "- `block`: The LD region where the gene’s transcription start site (TSS) is located, based on [predefined LD blocks](https://github.com/cumc/xqtl-data/blob/main/descriptor/reference_data/ld_reference.md). \n",
    "\n",
    "If `twas_z` is `NA` it means the context is not imputable for the method of choice\n",
    "\n",
    "II. a list of  `refined_twas_weights_data` per block, in RDS format, of this structure:\n",
    "\n",
    "```\n",
    "$ region_id\n",
    "   $ gene \n",
    "      $ context\n",
    "        $ selected_model\n",
    "        $ is_imputable \n",
    "        $ selected_top_variants\n",
    "        $ selected_model_weights\n",
    "\n",
    "```\n",
    "The twas result table will only contain imputatable genes. It should come with a meta-data file like this:\n",
    "\n",
    "```\n",
    "chrom    start    end    block_id  refined_twas_db\n",
    "1        1000     5000   block1    block1.rds\n",
    "2        2000     6000   block2    block2.rds\n",
    "3        3000     7000   block3    block3.rds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### iii. Run TWAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/twas_ctwas.ipynb twas \\\n",
    "   --cwd output/twas --name test \\\n",
    "   --gwas_meta_data data/twas/gwas_meta_test.tsv \\\n",
    "   --ld_meta_data reference_data/ADSP_R4_EUR/ld_meta_file.tsv \\\n",
    "   --regions data/twas/EUR_LD_blocks.bed \\\n",
    "   --xqtl_meta_data data/twas/mwe_twas_pipeline_test_small.tsv \\\n",
    "   --xqtl_type_table data/twas/data_type_table.txt \\\n",
    "   --rsq_pval_cutoff 0.05 --rsq_cutoff 0.01    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### iv. Run cTWAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/twas_ctwas.ipynb ctwas \\\n",
    "   --cwd output/twas --name test \\\n",
    "   --gwas_meta_data data/twas/gwas_meta_test.tsv \\\n",
    "   --ld_meta_data data/ld_meta_file_with_bim.tsv \\\n",
    "   --xqtl_meta_data data/twas/mwe_twas_pipeline_test_small.tsv \\\n",
    "   --twas_weight_cutoff 0 \\\n",
    "   --chrom 11 \\\n",
    "   --regions data/twas/EUR_LD_blocks.bed \\\n",
    "   --region-name chr10_80126158_82231647 chr11_84267999_86714492"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Here using `--region-name` we focus the analysis on 2 blocks: format as `chr_start_stop`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "```\n",
    "sos run xqtl-protocol/code/pecotmr_integration/twas.ipynb quantile_twas \\\n",
    "   --cwd /output/ --name test \\\n",
    "   --gwas_meta_data /mnt/vast/hpc/csg/cl4215/mrmash/workflow/GWAS/gwas_meta.tsv \\\n",
    "   --ld_meta_data /mnt/vast/hpc/csg/data_public/20240409_ADSP_LD_matrix/ld_meta_file.tsv \\\n",
    "   --region_name chr11_84267999_86714492 chr7_54681006_57314931 \\\n",
    "   --xqtl_meta_data /home/al4225/project/quantile_twas/quantile_twas_analysis/test_data/small_region_gene_meta_data_test.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "It is also possible to analyze a selected list of regions using option `--regions`. The 3 columns(chr, start, stop) of this file will be used for the block list to analyze. Here for example use the same list of regions as we used for LD block:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```\n",
    "sos run xqtl-protocol/code/pecotmr_integration/twas.ipynb quantile_twas \\\n",
    "   --cwd /output/ --name test \\\n",
    "   --gwas_meta_data /mnt/vast/hpc/csg/cl4215/mrmash/workflow/GWAS/gwas_meta.tsv \\\n",
    "   --ld_meta_data /mnt/vast/hpc/csg/data_public/20240409_ADSP_LD_matrix/ld_meta_file.tsv \\\n",
    "   --regions /mnt/vast/hpc/csg/cl4215/mrmash/workflow/pipeline_data/EUR_LD_blocks.bed \\\n",
    "   --xqtl_meta_data /home/al4225/project/quantile_twas/quantile_twas_analysis/test_data/small_region_gene_meta_data_test.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output/\")\n",
    "parameter: gwas_meta_data = path()\n",
    "parameter: xqtl_meta_data = path()\n",
    "parameter: ld_meta_data = path()\n",
    "parameter: xqtl_type_table = ''\n",
    "parameter: gwas_name = []\n",
    "parameter: gwas_data = []\n",
    "parameter: column_mapping = []\n",
    "parameter: regions = path()\n",
    "# Optional: if a region name is provided \n",
    "# the analysis would be focused on the union of provides region list and region names\n",
    "parameter: region_name = []\n",
    "parameter: name = f\"{xqtl_meta_data:bn}.{gwas_meta_data:bn}\"\n",
    "parameter: container = ''\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "parameter: job_size = 100\n",
    "parameter: walltime = \"5m\"\n",
    "parameter: mem = \"8G\"\n",
    "parameter: numThreads = 1\n",
    "# name suffix add to end of the name variable for the output files \n",
    "parameter: name_suffix = \"\"\n",
    "# optional parameter in ctwas to only perform gwas specific analysis \n",
    "parameter: gwas_study=[]\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adapt_file_path(file_path, reference_file):\n",
    "    \"\"\"\n",
    "    Adapt a single file path based on its existence and a reference file's path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The file path to adapt.\n",
    "    - reference_file (str): File path to use as a reference for adaptation.\n",
    "\n",
    "    Returns:\n",
    "    - str: Adapted file path.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If no valid file path is found.\n",
    "    \"\"\"\n",
    "    reference_path = os.path.dirname(reference_file)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        return file_path\n",
    "\n",
    "    # Check file name without path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    if os.path.isfile(file_name):\n",
    "        return file_name\n",
    "\n",
    "    # Check file name in reference file's directory\n",
    "    file_in_ref_dir = os.path.join(reference_path, file_name)\n",
    "    if os.path.isfile(file_in_ref_dir):\n",
    "        return file_in_ref_dir\n",
    "\n",
    "    # Check original file path prefixed with reference file's directory\n",
    "    file_prefixed = os.path.join(reference_path, file_path)\n",
    "    if os.path.isfile(file_prefixed):\n",
    "        return file_prefixed\n",
    "\n",
    "    # If all checks fail, raise an error\n",
    "    raise FileNotFoundError(f\"No valid path found for file: {file_path}\")\n",
    "\n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "[get_analysis_regions: shared = [\"filtered_region_info\", \"filtered_regional_xqtl_files\", \"regional_data\"]]\n",
    "from collections import OrderedDict\n",
    "\n",
    "def check_required_columns(df, required_columns):\n",
    "    \"\"\"Check if the required columns are present in the dataframe.\"\"\"\n",
    "    missing_columns = [col for col in required_columns if col not in list(df.columns)]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "def extract_regional_data(gwas_meta_data, xqtl_meta_data, regions, region_name, gwas_name, gwas_data, column_mapping):\n",
    "    \"\"\"\n",
    "    Extracts data from GWAS and xQTL metadata files and additional GWAS data provided. \n",
    "\n",
    "    Args:\n",
    "    - gwas_meta_data (str): File path to the GWAS metadata file.\n",
    "    - xqtl_meta_data (str): File path to the xQTL weight metadata file.\n",
    "    - gwas_name (list): vector of GWAS study names.\n",
    "    - gwas_data (list): vector of GWAS data.\n",
    "    - column_mapping (list, optional): vector of column mapping files.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of two dictionaries:\n",
    "        - GWAS Dictionary: Maps study IDs to a list containing chromosome number, \n",
    "          GWAS file path, and optional column mapping file path.\n",
    "        - xQTL Dictionary: Nested dictionary with region IDs as keys.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If any specified file path does not exist.\n",
    "    - ValueError: If required columns are missing in the input files or vector lengths mismatch.\n",
    "    \"\"\"\n",
    "    # Check vector lengths\n",
    "    if len(gwas_name) != len(gwas_data):\n",
    "        raise ValueError(\"gwas_name and gwas_data must be of equal length\")\n",
    "    \n",
    "    if len(column_mapping)>0 and len(column_mapping) != len(gwas_name):\n",
    "        raise ValueError(\"If column_mapping is provided, it must be of the same length as gwas_name and gwas_data\")\n",
    "\n",
    "    # Required columns for each file type\n",
    "    required_gwas_columns = ['study_id', 'chrom', 'file_path']\n",
    "    required_xqtl_columns = ['region_id', '#chr', 'start', 'end', \"TSS\", 'original_data'] #region_id here is gene name\n",
    "    required_ld_columns = ['chr', 'start', 'stop']\n",
    "    \n",
    "    # Reading the GWAS metadata file\n",
    "    gwas_df = pd.read_csv(gwas_meta_data, sep=\"\\t\")\n",
    "    check_required_columns(gwas_df, required_gwas_columns)\n",
    "    gwas_dict = OrderedDict()\n",
    "    \n",
    "    # Reading LD regions info\n",
    "    # Initialize empty DataFrame for regions\n",
    "    regions_df = pd.DataFrame(columns=['chr', 'start', 'stop'])\n",
    "\n",
    "    # Check if regions file exists and read it\n",
    "    if os.path.isfile(regions):\n",
    "        file_regions_df = pd.read_csv(regions, sep=\"\\t\", skipinitialspace=True)\n",
    "        file_regions_df.columns = [col.strip() for col in file_regions_df.columns]  # Strip spaces from column names\n",
    "        file_regions_df['chr'] = file_regions_df['chr'].str.strip()\n",
    "        check_required_columns(file_regions_df, required_ld_columns)\n",
    "        regions_df = pd.concat([regions_df, file_regions_df])\n",
    "    # Process region_name if provided: \n",
    "    # fomat: region_name = [\"chr1_16103_2888443\", \"chr1_4320284_5853833\"]\n",
    "    if len(region_name) > 0:\n",
    "        # Split region_name entries into chr, start, and stop columns\n",
    "        extra_regions = [name.split(\"_\") for name in region_name]\n",
    "        extra_regions_df = pd.DataFrame(extra_regions, columns=['chr', 'start', 'stop'])\n",
    "        extra_regions_df['start'] = extra_regions_df['start'].astype(int)\n",
    "        extra_regions_df['stop'] = extra_regions_df['stop'].astype(int)\n",
    "        # Add extra regions to regions_df\n",
    "        regions_df = pd.concat([regions_df, extra_regions_df])\n",
    "    # Remove duplicates and reset index\n",
    "    regions_df = regions_df.drop_duplicates().reset_index(drop=True)\n",
    "    regions_dict = OrderedDict()\n",
    "\n",
    "    # Reading the xQTL weight metadata file\n",
    "    xqtl_df = pd.read_csv(xqtl_meta_data, sep=\" \")\n",
    "    check_required_columns(xqtl_df, required_xqtl_columns)\n",
    "    xqtl_dict = OrderedDict()\n",
    "\n",
    "    # Process additional GWAS data from R vectors\n",
    "    for name, data, mapping in zip(gwas_name, gwas_data, column_mapping or [None]*len(gwas_name)):\n",
    "        gwas_dict[name] = {0: [data, mapping]}\n",
    "\n",
    "    for _, row in gwas_df.iterrows():\n",
    "        file_path = row['file_path']\n",
    "        mapping_file = row.get('column_mapping_file')\n",
    "        \n",
    "        # Adjust paths if necessary\n",
    "        file_path = adapt_file_path(file_path, gwas_meta_data)\n",
    "        if mapping_file:\n",
    "            mapping_file = adapt_file_path(mapping_file,  gwas_meta_data)\n",
    "\n",
    "       # Create or update the entry for the study_id\n",
    "        if row['study_id'] not in gwas_dict:\n",
    "            gwas_dict[row['study_id']] = {}\n",
    "\n",
    "        # Expand chrom 0 to chrom 1-22 or use the specified chrom\n",
    "        chrom_range = range(1, 23) if row['chrom'] == 0 else [row['chrom']]\n",
    "        for chrom in chrom_range:\n",
    "            if chrom in gwas_dict[row['study_id']]:\n",
    "                existing_entry = gwas_dict[row['study_id']][f'chr{chrom}']\n",
    "                raise ValueError(f\"Duplicate chromosome specification for study_id {row['study_id']}, chrom {chrom}. \"\n",
    "                                 f\"Conflicting entries: {existing_entry} and {[file_path, mapping_file]}\")\n",
    "            gwas_dict[row['study_id']][f'chr{chrom}'] = [file_path, mapping_file]\n",
    "            \n",
    "    for _, row in regions_df.iterrows():\n",
    "        LD_region_id = f\"{row['chr']}_{row['start']}_{row['stop']}\"\n",
    "        overlapping_xqtls = xqtl_df[(xqtl_df['#chr'] == row['chr']) & \n",
    "                                     (xqtl_df['TSS'] <= row['stop']) & \n",
    "                                     (xqtl_df['TSS'] >= (row['start']))]\n",
    "        file_paths = []\n",
    "        mapped_genes = []\n",
    "        # Collect file paths for xQTLs overlapping this region\n",
    "        for _, xqtl_row in overlapping_xqtls.iterrows():\n",
    "            original_data = xqtl_row['original_data']\n",
    "            file_list = original_data.split(',') if ',' in original_data else [original_data]\n",
    "            file_paths.extend([adapt_file_path(fp.strip(), xqtl_meta_data) for fp in file_list])\n",
    "            mapped_genes.extend([xqtl_row['region_id']] * len(file_list))\n",
    "\n",
    "        # Store metadata and files in the dictionary\n",
    "        regions_dict[LD_region_id] = {\n",
    "            \"meta_info\": [row['chr'], row['start'], row['stop'], LD_region_id, mapped_genes],\n",
    "            \"files\": file_paths\n",
    "        }\n",
    "        \n",
    "    for _, row in xqtl_df.iterrows():\n",
    "        file_paths = [adapt_file_path(fp.strip(), xqtl_meta_data) for fp in row['original_data'].split(',')]  # Splitting and stripping file paths\n",
    "        xqtl_dict[row['region_id']] = {\"meta_info\": [row['#chr'], row['start'], row['end'], row['region_id'], row['contexts']],\n",
    "                                       \"files\": file_paths}\n",
    "    return gwas_dict, xqtl_dict, regions_dict\n",
    "\n",
    "\n",
    "gwas_dict, xqtl_dict, regions_dict = extract_regional_data(gwas_meta_data, xqtl_meta_data,regions,region_name,gwas_name, gwas_data, column_mapping)\n",
    "regional_data = dict([(\"GWAS\", gwas_dict), (\"xQTL\", xqtl_dict), (\"Regions\", regions_dict)])\n",
    "\n",
    "\n",
    "# get regions data \n",
    "region_info = [x[\"meta_info\"] for x in regional_data['Regions'].values()]\n",
    "regional_xqtl_files = [x[\"files\"] for x in regional_data['Regions'].values()]\n",
    "\n",
    "# Filter out empty xQTL file paths\n",
    "filtered_region_info = []\n",
    "filtered_regional_xqtl_files = []\n",
    "skipped_regions =[]\n",
    "\n",
    "for region, files in zip(region_info, regional_xqtl_files):\n",
    "    if files:\n",
    "        filtered_region_info.append(region)\n",
    "        filtered_regional_xqtl_files.append(files)\n",
    "    else:\n",
    "        skipped_regions.append(region)\n",
    "print(f\"Skipping {len(skipped_regions)} out of {len(regional_xqtl_files)} regions, no overlapping xQTL weights found. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[twas]\n",
    "depends: sos_variable(\"filtered_regional_xqtl_files\")\n",
    "parameter: coverage = \"cs_coverage_0.95\"\n",
    "# Threshold for rsq and pvalue for imputability determination for a model \n",
    "parameter: rsq_cutoff = 0.01\n",
    "parameter: rsq_pval_cutoff = 0.05\n",
    "parameter: mr_pval_cutoff = 0.05\n",
    "parameter: save_ctwas_data = True\n",
    "parameter: save_mr_result = True\n",
    "parameter: rsq_option=\"rsq\"\n",
    "parameter: rsq_pval_option=[\"adj_rsq_pval\", \"pval\"]\n",
    "# load by batches if memory resource is limited, default to load all at once \n",
    "parameter: batch_load_memory = 500\n",
    "parameter: event_filter_rules = path()\n",
    "parameter: comment_string = \"NULL\"\n",
    "parameter: rename_column = False\n",
    "input: filtered_regional_xqtl_files, group_by = lambda x: group_by_region(x, filtered_regional_xqtl_files), group_with = \"filtered_region_info\"\n",
    "output_files = [f'{cwd:a}/{step_name}/{name}.{_filtered_region_info[3]}.twas.tsv.gz']\n",
    "if save_ctwas_data:\n",
    "    output_files.append(f'{cwd:a}/{step_name}/{name}.{_filtered_region_info[3]}.twas_data.rds')\n",
    "if save_mr_result:\n",
    "    output_files.append(f'{cwd:a}/{step_name}/{name}.{_filtered_region_info[3]}.mr_result.tsv.gz')\n",
    "output: output_files\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]:n}.stdout\", stderr = f\"{_output[0]:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "\n",
    "    library(dplyr)\n",
    "    library(data.table)\n",
    "    library(pecotmr)\n",
    "    library(readr)\n",
    "\n",
    "\n",
    "\n",
    "    # Load metadata and configuration - let these fail if there are issues\n",
    "    if (${\"TRUE\" if rename_column else \"FALSE\"}) {\n",
    "        gwas_meta_data = fread(\"${gwas_meta_data}\", data.table=FALSE)\n",
    "        column_file_path = gwas_meta_data$column_mapping[1]\n",
    "    } else {\n",
    "        column_file_path = NULL\n",
    "    }\n",
    "    \n",
    "    xqtl_meta_df <- fread(\"${xqtl_meta_data}\", data.table=FALSE)\n",
    "    xqtl_type_table <- if (isTRUE(file.exists(\"${xqtl_type_table}\"))) fread(\"${xqtl_type_table}\") else NULL\n",
    "    gene_list <- c(${', '.join([f\"'{gene}'\" for gene in _filtered_region_info[4]])})\n",
    "    \n",
    "    event_filter_rules = ${\"NULL\" if not event_filter_rules.is_file() else \"'%s'\" % event_filter_rules}\n",
    "    if (!is.null(event_filter_rules)) {\n",
    "        event_filters <- read.table(\"${event_filter_rules}\")\n",
    "        event_filters <- lapply(1:nrow(event_filters), function(ii) as.list(event_filters[ii,] %>% unlist))\n",
    "    } else { \n",
    "        event_filters <- NULL \n",
    "    }\n",
    "\n",
    "    # Process weights with targeted error handling for file operations\n",
    "    weight_db_list <- c(${_input:r,})\n",
    "    names(weight_db_list) <- gene_list\n",
    "    weight_db_list <- split(weight_db_list, names(weight_db_list))\n",
    "    \n",
    "    # Filter out empty/invalid weight files with error handling\n",
    "    weight_db_list_update <- tryCatch({\n",
    "        Filter(Negate(is.null), lapply(weight_db_list, function(file_list) {\n",
    "            do.call(c, lapply(file_list, function(file) {\n",
    "                if (file.exists(file) && file.size(file) > 200) file else NULL\n",
    "            }))\n",
    "        }))\n",
    "    }, error = function(e) {\n",
    "        message(paste(\"Error checking weight files:\", e$message))\n",
    "        return(list())\n",
    "    })\n",
    "    \n",
    "    if(length(weight_db_list_update) <= 0) {\n",
    "        message(paste0(\"No valid weight files for region ${_filtered_region_info[3]}. Creating empty output files.\"))\n",
    "        # Define TWAS result columns (same as in final_results)\n",
    "        twas_header <- data.frame(\n",
    "            gene = character(),\n",
    "            molecular_id = character(),\n",
    "            TSS = numeric(),\n",
    "            start = numeric(),\n",
    "            end = numeric(),\n",
    "            beta = numeric(),\n",
    "            se = numeric(),\n",
    "            z = numeric(),\n",
    "            pval = numeric(),\n",
    "            qval = numeric(),\n",
    "            rsq = numeric(),\n",
    "            mr_beta = numeric(),\n",
    "            mr_se = numeric(),\n",
    "            mr_pval = numeric()\n",
    "        )\n",
    "\n",
    "        # Write empty file with header, gzipped\n",
    "        fwrite(twas_header, file = ${_output[0]:r}, sep = \"\\t\", compress = \"gzip\")\n",
    "        if (${\"TRUE\" if save_ctwas_data else \"FALSE\"}) {\n",
    "            saveRDS(list(), \"${_output[0]:nnn}.twas_data.rds\", compress='xz')\n",
    "        }\n",
    "        if (${\"TRUE\" if save_mr_result else \"FALSE\"}) {\n",
    "            fwrite(data.frame(), file = \"${_output[0]:nnn}.mr_result.tsv.gz\", sep = \"\\t\", compress = \"gzip\")\n",
    "        }\n",
    "        quit(save = \"no\", status = 0)\n",
    "    }\n",
    "\n",
    "    # Load TWAS weights - allow this to fail with informative errors\n",
    "    message(paste(\"Loading TWAS weights for\", length(weight_db_list_update), \"genes\"))\n",
    "    twas_weights_results <- list()\n",
    "    \n",
    "    for (gene_db in names(weight_db_list_update)) {\n",
    "        weight_dbs <- weight_db_list_update[[gene_db]]\n",
    "        message(paste(\"Processing gene:\", gene_db, \"with\", length(weight_dbs), \"weight files\"))\n",
    "        \n",
    "        # Load weights for this gene - let it fail if there are real issues\n",
    "        twas_weights_results[[gene_db]] = load_twas_weights(\n",
    "            weight_dbs, \n",
    "            variable_name_obj = \"variant_names\", \n",
    "            susie_obj = \"susie_weights_intermediate\",\n",
    "            twas_weights_table = \"twas_weights\"\n",
    "        )\n",
    "        \n",
    "        if (length(twas_weights_results[[gene_db]]) > 1) {\n",
    "            twas_weights_results[[gene_db]]$data_type <- setNames(\n",
    "                lapply(names(twas_weights_results[[gene_db]]$weights), function(context) {\n",
    "                    xqtl_type_table$type[sapply(xqtl_type_table$context, function(x) grepl(x, context))]\n",
    "                }), \n",
    "                names(twas_weights_results[[gene_db]]$weights)\n",
    "            )      \n",
    "        } else {\n",
    "            message(paste(\"No valid weights loaded for gene:\", gene_db))\n",
    "            twas_weights_results[[gene_db]] <- NULL\n",
    "        }                   \n",
    "    }\n",
    "    \n",
    "    # Clean up and check if we have any valid results\n",
    "    rm(weight_db_list, weight_db_list_update, xqtl_type_table, gene_list)\n",
    "    gc()\n",
    "    \n",
    "    # Filter out NULL results and report\n",
    "    valid_genes <- names(Filter(Negate(is.null), twas_weights_results))\n",
    "    message(paste(\"Valid weights loaded for\", length(valid_genes), \"genes:\", paste(valid_genes, collapse = \", \")))\n",
    "    \n",
    "    if (length(valid_genes) == 0) {\n",
    "        message(paste0(\"No valid TWAS weights loaded for region ${_filtered_region_info[3]}. Creating empty output files.\"))\n",
    "        fwrite(data.frame(), file = ${_output[0]:r}, sep = \"\\t\", compress = \"gzip\")\n",
    "        if (${\"TRUE\" if save_ctwas_data else \"FALSE\"}) {\n",
    "            saveRDS(list(), \"${_output[0]:nnn}.twas_data.rds\", compress='xz')\n",
    "        }\n",
    "        if (${\"TRUE\" if save_mr_result else \"FALSE\"}) {\n",
    "            fwrite(data.frame(), file = \"${_output[0]:nnn}.mr_result.tsv.gz\", sep = \"\\t\", compress = \"gzip\")\n",
    "        }\n",
    "        quit(save = \"no\", status = 0)\n",
    "    }\n",
    "\n",
    "    # Batch load weights - allow this to fail\n",
    "    twas_weights_results <- batch_load_twas_weights(\n",
    "        twas_weights_results = twas_weights_results,\n",
    "        meta_data_df = xqtl_meta_df,\n",
    "        max_memory_per_batch = ${\"Inf\" if batch_load_memory == \"inf\" else batch_load_memory}\n",
    "    )\n",
    "    \n",
    "    message(paste(\"Proceeding with TWAS analysis for\", length(twas_weights_results), \"batches\"))\n",
    "\n",
    "    # TWAS analysis - allow this to fail with informative errors\n",
    "    twas_results_db <- list()\n",
    "    for (batch in 1:length(twas_weights_results)){\n",
    "        message(paste(\"Processing batch\", batch, \"of\", length(twas_weights_results)))\n",
    "        \n",
    "        twas_results_db[[batch]] <- twas_pipeline(\n",
    "            twas_weights_results[[batch]], \n",
    "            \"${ld_meta_data}\", \n",
    "            \"${gwas_meta_data}\",  \n",
    "            region_block = \"${_filtered_region_info[3]}\",\n",
    "            rsq_cutoff = ${rsq_cutoff}, \n",
    "            rsq_option = \"${rsq_option}\", \n",
    "            rsq_pval_cutoff = ${rsq_pval_cutoff}, \n",
    "            rsq_pval_option = c(${\", \".join([f'\"{x}\"' for x in rsq_pval_option])}), \n",
    "            mr_pval_cutoff = ${mr_pval_cutoff},\n",
    "            mr_coverage_column = \"${coverage}\", \n",
    "            output_twas_data = ${\"TRUE\" if save_ctwas_data else \"FALSE\"},\n",
    "            event_filters = event_filters,\n",
    "            column_file_path = column_file_path,\n",
    "            comment_string = ${\"NULL\" if comment_string == \"NULL\" else f\"'{comment_string}'\"}\n",
    "        )\n",
    "        \n",
    "        # Report batch results\n",
    "        if (!is.null(twas_results_db[[batch]])) {\n",
    "            if (!is.null(twas_results_db[[batch]]$twas_result)) {\n",
    "                message(paste(\"Batch\", batch, \"produced\", nrow(twas_results_db[[batch]]$twas_result), \"TWAS results\"))\n",
    "            } else {\n",
    "                message(paste(\"Batch\", batch, \"produced NULL twas_result\"))\n",
    "            }\n",
    "        } else {\n",
    "            message(paste(\"Batch\", batch, \"produced NULL results\"))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    rm(twas_weights_results)\n",
    "    gc()\n",
    "    \n",
    "    # Filter and report final results\n",
    "    twas_results_db <- Filter(Negate(is.null), twas_results_db)\n",
    "    message(paste(\"Final valid batches:\", length(twas_results_db)))\n",
    "\n",
    "    if(length(twas_results_db) != 0){\n",
    "        # Merge with metadata\n",
    "        for (batch in 1:length(twas_results_db)){\n",
    "            if (!is.null(twas_results_db[[batch]]$twas_result)) {\n",
    "                twas_results_db[[batch]]$twas_result <- merge(\n",
    "                    twas_results_db[[batch]]$twas_result, \n",
    "                    xqtl_meta_df[, c(\"region_id\",\"TSS\",\"start\",\"end\")], \n",
    "                    by.x = \"molecular_id\", by.y = \"region_id\"\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Combine and write results\n",
    "        final_results <- do.call(rbind, lapply(twas_results_db, function(x) {\n",
    "            if (!is.null(x$twas_result)) x$twas_result[, c(2,1,14:16,3:13)] else data.frame()\n",
    "        }))\n",
    "        \n",
    "        message(paste(\"Writing\", nrow(final_results), \"final TWAS results\"))\n",
    "        fwrite(final_results, ${_output[0]:r}, sep = \"\\t\", compress = \"gzip\")\n",
    "    } else {\n",
    "        message(\"No valid TWAS results to write\")\n",
    "        fwrite(data.frame(), file = ${_output[0]:r}, sep = \"\\t\", compress = \"gzip\")\n",
    "    }\n",
    "\n",
    "    # Save cTWAS data if requested\n",
    "    if (${\"TRUE\" if save_ctwas_data else \"FALSE\"} & length(twas_results_db) != 0) {\n",
    "        if (length(twas_results_db) > 1){\n",
    "            # Combine data from multiple batches\n",
    "            twas_results_db[[1]]$twas_data$weights <- do.call(c, lapply(twas_results_db, function(x) x$twas_data$weights))\n",
    "            twas_results_db[[1]]$twas_data$susie_weights_intermediate_qced <- do.call(c, lapply(twas_results_db, function(x) x$twas_data$susie_weights_intermediate_qced))\n",
    "            studies <- unique(names(find_data(twas_results_db, c(3, \"z_gene\"))))\n",
    "            for (study in studies){\n",
    "                twas_results_db[[1]][[\"twas_data\"]][[\"z_gene\"]][[study]] <- do.call(rbind, lapply(twas_results_db, function(x) x$twas_data$z_gene[[study]]))\n",
    "            }\n",
    "        }\n",
    "        saveRDS(twas_results_db[[1]]$twas_data, \"${_output[0]:nnn}.twas_data.rds\", compress='xz')\n",
    "    } else if (${\"TRUE\" if save_ctwas_data else \"FALSE\"}) {\n",
    "        saveRDS(list(), \"${_output[0]:nnn}.twas_data.rds\", compress='xz')\n",
    "    }\n",
    "\n",
    "    # Save MR results if requested\n",
    "    if (${\"TRUE\" if save_mr_result else \"FALSE\"}) {\n",
    "        if(length(twas_results_db) != 0) {\n",
    "            mr_results <- do.call(rbind, lapply(twas_results_db, function(x) x$mr_result))\n",
    "            message(paste(\"Writing\", nrow(mr_results), \"MR results\"))\n",
    "            fwrite(mr_results, file = \"${_output[0]:nnn}.mr_result.tsv.gz\", sep = \"\\t\", compress = \"gzip\")\n",
    "        } else {\n",
    "            fwrite(data.frame(), file = \"${_output[0]:nnn}.mr_result.tsv.gz\", sep = \"\\t\", compress = \"gzip\")\n",
    "        }\n",
    "    }\n",
    "\n",
    "    message(\"TWAS analysis completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "[ctwas_1]\n",
    "# ctwas_1: merge regions by each chromosome \n",
    "depends: sos_variable(\"filtered_region_info\")\n",
    "# chromosome number to merge for region data, can be an integer or a string, i.e. 'chr1'\n",
    "parameter: chrom=\"\"\n",
    "# twas weight cutoff for ctwas variant selection\n",
    "parameter: twas_weight_cutoff = 0.0\n",
    "parameter: max_num_variants =\"Inf\"\n",
    "# cs_min_cor cutoff in susie finemapping result for variant selection\n",
    "parameter: cs_min_cor = 0\n",
    "# minimum pip cutoff in susie finemapping result for variant selection\n",
    "parameter: min_pip_cutoff =0 \n",
    "# A scalar in (0,1]. The proportion of SNPs, reduce runtime at the cost of accuracy \n",
    "parameter: thin = 1.0\n",
    "# Maximum number of SNPs in a region.\n",
    "parameter: maxSNP = 20000\n",
    "# skip this step if True\n",
    "parameter: skip_assembly = False\n",
    "skip_if(skip_assembly == True, \" Skip [ctwas_1] assemble regions. \" )\n",
    "parameter: multi_group = True\n",
    "# A string character add to the end of name variable\n",
    "parameter: numThreads = 4\n",
    "\n",
    "twas_output_files = {}\n",
    "region_blocks_per_chrom = []\n",
    "chromosome_list = []\n",
    "chrom = f\"chr{int(chrom)}\" if str(chrom).isdigit() else chrom\n",
    "for region_info in filtered_region_info:\n",
    "    chrom_info = region_info[0]\n",
    "    if chrom_info != chrom:\n",
    "        continue  # Skip chromosomes other than chrom input \n",
    "    file_path = f\"{cwd}/twas/{name}.{region_info[3]}.twas_data.rds\"\n",
    "    if chrom_info not in twas_output_files:\n",
    "        twas_output_files[chrom_info] = [[], []]  # Structure: {\"chrX\": [[files], [region_names]]}\n",
    "        chromosome_list.append(chrom_info)\n",
    "    twas_output_files[chrom_info][0].append(file_path)  # File paths\n",
    "    twas_output_files[chrom_info][1].append(region_info[3])  # Region names\n",
    "twas_files=[twas_output_files[chr][0] for chr in chromosome_list]\n",
    "\n",
    "region_blocks_per_chrom = [twas_output_files[chr][1] for chr in chromosome_list]\n",
    "region_blocks_per_chrom = [f\"\"\"c(\"{'\", \"'.join(regions)}\")\"\"\" for regions in region_blocks_per_chrom]\n",
    "outdir = f\"{cwd}/{step_name.split('_')[0]}\"\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "gwas_study = 'c(' + ', '.join(f'\"{x}\"' for x in gwas_study) + ')'\n",
    "name_suffix = f'\"{name_suffix}\"' if name_suffix else 'NULL'\n",
    "\n",
    "input: twas_files, group_by = lambda x: group_by_region(x, twas_files),  group_with=dict(region_names=region_blocks_per_chrom)\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "R: expand = '${ }', stdout = f\"{cwd:a}/{step_name.split('_')[0]}/{name}.ctwas_region_data.{chrom}.thin{thin}.stdout\", \n",
    "stderr = f\"{cwd:a}/{step_name.split('_')[0]}/{name}.ctwas_region_data.{chrom}.thin{thin}.stderr\", container = container, entrypoint = entrypoint\n",
    "\n",
    "    library(data.table)\n",
    "    library(ctwas) # multigroup_ctwas\n",
    "    library(pecotmr)\n",
    "    library(stringr)\n",
    "    outputdir = \"${cwd:a}/${step_name.split('_')[0]}\"\n",
    "    \n",
    "    # load genome-wide data across all regions\n",
    "    gwas_meta_data <- fread(\"${gwas_meta_data}\",data.table=FALSE)\n",
    "    gwas_studies <- if (length(${gwas_study})!=0) ${gwas_study} else unique(gwas_meta_data$study_id)\n",
    "    gwas_files <- gwas_meta_data[gwas_meta_data$chrom == readr::parse_number(\"${chrom}\") & gwas_meta_data$study_id %in% gwas_studies,, drop=FALSE]\n",
    "    gwas_files <- setNames(gwas_files$file_path, gwas_files$study_id)\n",
    "\n",
    "    merge_context_names <- function(ctwas_dat_file){\n",
    "        ctwas_dat<- readRDS(ctwas_dat_file)\n",
    "        idxs <- which(grepl(\"_chr[0-9XY]+_[^_]+\", names(ctwas_dat$weights)))\n",
    "        if (length(idxs)>0) {\n",
    "            names(ctwas_dat$weights) <- gsub(\"_chr[0-9XY]+_[A-Za-z0-9]+\", \"\", names(ctwas_dat$weights))\n",
    "            for (idx in idxs) {\n",
    "                for (study in names(ctwas_dat$weights[[idx]])){\n",
    "                    ctwas_dat$weights[[idx]][[study]]$weight_name <- gsub(\"_chr[0-9XY]+_[A-Za-z0-9]+\", \"\", ctwas_dat$weights[[idx]][[study]]$weight_name)\n",
    "                    ctwas_dat$weights[[idx]][[study]]$context <- gsub(\"_chr[0-9XY]+_[A-Za-z0-9]+\", \"\", ctwas_dat$weights[[idx]][[study]]$context)\n",
    "                }\n",
    "            }\n",
    "            for (study in names(ctwas_dat$z_gene)){\n",
    "                ctwas_dat$z_gene[[study]]$id <- gsub(\"_chr[0-9XY]+_[A-Za-z0-9]+\", \"\", ctwas_dat$z_gene[[study]]$id)\n",
    "                ctwas_dat$z_gene[[study]]$context <- gsub(\"_chr[0-9XY]+_[A-Za-z0-9]+\", \"\", ctwas_dat$z_gene[[study]]$context)\n",
    "                ctwas_dat$z_gene[[study]]$group <- gsub(\"_chr[0-9XY]+_[A-Za-z0-9]+\", \"\", ctwas_dat$z_gene[[study]]$group)\n",
    "            }\n",
    "            for (gene in names(ctwas_dat$susie_weights_intermediate_qced)){\n",
    "                names(ctwas_dat$susie_weights_intermediate_qced[[gene]]) <-  gsub(\"_chr[0-9XY]+_[^_]+\", \"\", names(ctwas_dat$susie_weights_intermediate_qced[[gene]]))\n",
    "            }\n",
    "        }\n",
    "        idxs <- which(grepl(\"monocyte\", names(ctwas_dat$weights)))\n",
    "        if (length(idxs)>0) {\n",
    "            names(ctwas_dat$weights) <- gsub(\"monocyte_eQTL\", \"eQTL\", names(ctwas_dat$weights))\n",
    "            for (idx in idxs) {\n",
    "                for (study in names(ctwas_dat$weights[[idx]])){\n",
    "                    ctwas_dat$weights[[idx]][[study]]$weight_name <- gsub(\"monocyte_eQTL\", \"eQTL\", ctwas_dat$weights[[idx]][[study]]$weight_name)\n",
    "                    ctwas_dat$weights[[idx]][[study]]$type <- \"eQTL\"\n",
    "                }\n",
    "            }\n",
    "            for (study in names(ctwas_dat$z_gene)){\n",
    "                ctwas_dat$z_gene[[study]]$id <- gsub(\"monocyte_eQTL\", \"eQTL\", ctwas_dat$z_gene[[study]]$id)\n",
    "                ctwas_dat$z_gene[[study]]$type <- gsub(\"monocyte_\", \"\", ctwas_dat$z_gene[[study]]$type)\n",
    "                ctwas_dat$z_gene[[study]]$group <- gsub(\"monocyte_eQTL\", \"eQTL\", ctwas_dat$z_gene[[study]]$group)\n",
    "            }\n",
    "        }\n",
    "        return(ctwas_dat)\n",
    "    }\n",
    "    weight_list = vector(\"list\", length(gwas_studies))\n",
    "    names(weight_list) <- gwas_studies\n",
    "    z_gene <- weight_list  # empty list  \n",
    "    z_snp <- weight_list\n",
    "    \n",
    "    # get LD snp info table (snp_map) and ld variants \n",
    "    max_pos <- fread(\"${ld_meta_data}\", data.table=FALSE)\n",
    "    max_pos <- max(max_pos$end[max_pos[,1]==\"${chrom}\"])\n",
    "    region_of_interest <- data.frame(chrom = \"${chrom}\", start = 0, end = max_pos+1)\n",
    "    snp_map <- load_ld_snp_info(\"${ld_meta_data}\", region_of_interest)\n",
    "    ld_variants <- paste0(\"chr\", unlist(lapply(snp_map, function(x) x$id)))\n",
    "    ld_variants <- ld_variants[!duplicated(ld_variants)]\n",
    "\n",
    "    regions_data <- get_ctwas_meta_data(\"${ld_meta_data}\", names(snp_map))\n",
    "    region_info <- regions_data$region_info\n",
    "    LD_map <- regions_data$LD_info\n",
    "    if (!is.null(${name_suffix})) name <- paste0(\"${name}_\", ${name_suffix})  else name <- \"${name}\"\n",
    "    if (!file.exists(file.path(outputdir, paste0(name, \".LD_map.rds\")))) saveRDS(LD_map, file.path(outputdir, paste0(name, \".LD_map.rds\")), compress='xz')\n",
    "    saveRDS(snp_map, file.path(outputdir, paste0(name, \".snp_map.${chrom}.rds\")), compress='xz')\n",
    "    rm(regions_data, LD_map)\n",
    "    gc()\n",
    "    region_map <- c()\n",
    "    # merge chromosome wide data, weight, z_gene, z_snp from the twas pipeline output of twas.data.rds files \n",
    "    for (region_dat in c(${_input:r,})){\n",
    "        ctwas_dat <- merge_context_names(region_dat)\n",
    "        if (is.null(ctwas_dat)) next \n",
    "        weight_tmp <- trim_ctwas_variants(ctwas_dat, twas_weight_cutoff=${twas_weight_cutoff}, cs_min_cor=${cs_min_cor},\n",
    "                            min_pip_cutoff=${min_pip_cutoff}, max_num_variants=${max_num_variants})\n",
    "        if (length(weight_tmp)==0) next \n",
    "        for (study in names(weight_tmp)) {\n",
    "            if (!study %in% gwas_studies) next\n",
    "            weight_list[[study]] <- c(weight_list[[study]], weight_tmp[[study]])\n",
    "            z_gene[[study]] <- rbind(z_gene[[study]], ctwas_dat$z_gene[[study]])\n",
    "            z_gene[[study]] <- z_gene[[study]][!is.na(z_gene[[study]]$z) & !is.infinite(z_gene[[study]]$z) & z_gene[[study]]$id %in% names(weight_list[[study]]),]        \n",
    "        }\n",
    "        region <- str_extract(region_dat, \"chr[0-9XY]+_[0-9]+_[0-9]+\")\n",
    "        z_genes <- unique(find_data(weight_tmp, c(3, \"molecular_id\")))\n",
    "        names(z_genes) <- rep(str_extract(region_dat, \"chr[0-9XY]+_[0-9]+_[0-9]+\"), length(z_genes))\n",
    "        region_map <- c(region_map, z_genes)\n",
    "        rm(ctwas_dat, weight_tmp, z_genes, region)\n",
    "        gc()\n",
    "    }\n",
    "    z_gene <- Filter(Negate(is.null), z_gene)\n",
    "    weight_list <- Filter(Negate(is.null), weight_list)\n",
    "    if (length(z_gene)==0 | length(weight_list)==0) stop(\"Please check input data. No twas z-score value available. \")\n",
    "    \n",
    "    for (study in names(weight_list)){\n",
    "        z_snp[[study]] <- harmonize_gwas(gwas_files[study], query_region=paste0(readr::parse_number(\"${chrom}\"), \":0-\", max_pos+1), ld_variants, c(\"beta\", \"z\"), match_min_prop = 0)\n",
    "        z_snp[[study]] <- z_snp[[study]][, colnames(z_snp[[study]])[colnames(z_snp[[study]]) %in% c(\"chrom\", \"pos\", \"variant_id\", \"A1\", \"A2\", \"z\", \"beta\", \"n_sample\",\"n_case\", \"n_control\", \"effect_allele_frequency\")]]\n",
    "        z_snp[[study]] <- z_snp[[study]][, c(\"chrom\", \"pos\", colnames(z_snp[[study]])[!colnames(z_snp[[study]]) %in% c(\"chrom\", \"pos\")])]\n",
    "    }\n",
    "    # if weight variants are trimmed, load LD and re-compute twas z score \n",
    "    if (${twas_weight_cutoff}!=0 | ${cs_min_cor}!=0 | ${min_pip_cutoff}!=0 | ${max_num_variants}!=Inf ) {\n",
    "        region_map_list <- split(region_map, names(region_map))\n",
    "        for (region in names(region_map_list)){\n",
    "            region_genes <- unique(region_map_list[[region]])\n",
    "            weight_list_sub <- find_data(weight_list, 2)  \n",
    "            weight_list_sub <- weight_list_sub[gsub( \"\\\\|.*$\", \"\", names(weight_list_sub)) %in% region_genes]\n",
    "            region_of_interest <- data.frame(chrom = \"${chrom}\", start = min(find_data(weight_list_sub, c(2, \"p0\"))), end =  max(find_data(weight_list_sub, c(2, \"p1\"))))\n",
    "            rm(weight_list_sub);gc()\n",
    "            LD_list <- load_LD_matrix(\"${ld_meta_data}\", region_of_interest)\n",
    "            dup_idx <- which(duplicated(LD_list$combined_LD_variants))\n",
    "            LD_list <- LD_list['combined_LD_matrix']  \n",
    "            if (length(dup_idx) >= 1) LD_list$combined_LD_matrix <- LD_list$combined_LD_matrix[-dup_idx, -dup_idx]\n",
    "            for (study in names(weight_list)){\n",
    "                sub_idx <- which(gsub(\"\\\\|.*$\", \"\", names(weight_list[[study]])) %in% region_genes)\n",
    "                for (context in names(weight_list[[study]][sub_idx])){\n",
    "                    twas_df <- twas_analysis(weight_list[[study]][[context]]$wgt, z_snp[[study]], LD_list$combined_LD_matrix, rownames(weight_list[[study]][[context]]$wgt))[[1]]\n",
    "                    z_gene[[study]]$z[z_gene[[study]]$id==context &z_gene[[study]]$gwas_study==study] <- twas_df$z \n",
    "                }\n",
    "            }\n",
    "            rm(LD_list);gc()     \n",
    "        }\n",
    "        rm(LD_list);gc()\n",
    "    }\n",
    "\n",
    "    # for each gwas study - merge ctwas input data        \n",
    "    for (study in names(weight_list)){\n",
    "      if (is.null(z_gene[[study]])) next\n",
    "      if (${\"TRUE\" if multi_group else \"FALSE\"}) {\n",
    "          outname <- paste0(name, \".ctwas_region_data.\", study, \".${chrom}\")\n",
    "      } else {\n",
    "         contexts <- unique(do.call(c,lapply(z_gene, function(x) unique(x$context))))\n",
    "         outname <- paste0(name, \".ctwas_region_data.\", study, \".\", contexts, \".${chrom}\")\n",
    "         names(outname) <- contexts\n",
    "      }\n",
    "\n",
    "      colnames(z_snp[[study]])[match(\"variant_id\", colnames(z_snp[[study]]))] <-\"id\" \n",
    "      z_snp[[study]] <- z_snp[[study]][, c(\"id\", \"A1\", \"A2\", \"z\")]\n",
    "\n",
    "      # iterate by study-context pair (single group) or by study itself (multigroup) \n",
    "      for (group_name in outname){ \n",
    "          # assemble regions         \n",
    "          if (${\"TRUE\" if multi_group else \"FALSE\"}) {\n",
    "              subset_inx <- names(weight_list[[study]])\n",
    "          } else {\n",
    "              context <- names(outname)[which(outname==group_name)]\n",
    "              if (length(weight_list[[study]][grepl(context, names(weight_list[[study]]))])==0) next\n",
    "              subset_inx <- which(grepl(context, names(weight_list[[study]])))\n",
    "          }\n",
    "\n",
    "          res <- assemble_region_data(region_info,\n",
    "                                    z_snp[[study]],\n",
    "                                    z_gene[[study]],\n",
    "                                    weight_list[[study]][subset_inx],\n",
    "                                    snp_map,\n",
    "                                    thin = ${thin},\n",
    "                                    maxSNP = ${maxSNP},\n",
    "                                    min_group_size = 1,\n",
    "                                    ncore = ${numThreads})\n",
    "          region_data <- res$region_data\n",
    "          boundary_genes <- res$boundary_genes\n",
    "          region_data_file <- file.path(outputdir, paste0(group_name, \".thin\", ${thin}, \".rds\"))\n",
    "          boundary_gene_file <- gsub(\"region_data\",\"boundary_genes\", region_data_file)\n",
    "          saveRDS(region_data, region_data_file, compress='xz')\n",
    "          saveRDS(boundary_genes, boundary_gene_file, compress='xz')   \n",
    "      }\n",
    "      saveRDS(weight_list[[study]], file.path(outputdir, paste0(name,\".ctwas_weights.\", study, \".${chrom}.rds\")), compress='xz')\n",
    "      saveRDS(list(z_snp=z_snp[[study]], z_gene=z_gene[[study]]), file.path(outputdir,  paste0(name, \".z_gene_snp.\", study, \".${chrom}.rds\")), compress='xz')\n",
    "      weight_list[[study]] <- NULL\n",
    "      z_gene[[study]] <- NULL\n",
    "      z_snp[[study]] <- NULL\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "[ctwas_2]\n",
    "## ctwas_2: estimate global group prior using from all regions \n",
    "parameter: run_param_est = False\n",
    "parameter: skip_assembly = False\n",
    "parameter: thin=1.0\n",
    "parameter: prior_var_structure = \"shared_all\"\n",
    "parameter: multi_group = True\n",
    "parameter: numThreads = 8\n",
    "parameter: niter = 50\n",
    "\n",
    "thin=int(thin) if thin == 1.0 else thin\n",
    "skip_if(run_param_est == False, \" Skip [ctwas_2] parameter estimation. \" )\n",
    "name = f\"{name}_{name_suffix}\" if name_suffix else name\n",
    "\n",
    "gwas_study = 'c(' + ', '.join(f'\"{x}\"' for x in gwas_study) + ')'\n",
    "if multi_group:\n",
    "    input_pattern = f\"{cwd:a}/{step_name.split('_')[0]}/{name}.ctwas_region_data.*.chr*.thin{thin}.rds\" # by study\n",
    "else:\n",
    "    input_pattern = f\"{cwd:a}/{step_name.split('_')[0]}/{name}.ctwas_region_data.*.*.chr*.thin{thin}.rds\" # by study-context pair\n",
    "\n",
    "input: input_pattern, group_by=\"all\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "R: expand = '${ }', stdout = f\"{cwd:a}/{step_name.split('_')[0]}/{name}.ctwas_param_est.{prior_var_structure}.thin{thin}.stdout\", \n",
    "stderr = f\"{cwd:a}/{step_name.split('_')[0]}/{name}.ctwas_param_est.{prior_var_structure}.thin{thin}.stderr\", container = container, entrypoint = entrypoint\n",
    "    \n",
    "    library(ctwas)\n",
    "    library(data.table)\n",
    "    library(pecotmr)\n",
    "    outputdir = \"${cwd:a}/${step_name.split('_')[0]}\"\n",
    "    region_data_files <- c(${_input:r,})\n",
    "    names(region_data_files) <- gsub('^.*${name}.ctwas_region_data.\\\\s*|\\\\s*.chr*.*$', '', region_data_files)   \n",
    "    gwas_studies <- if (length(${gwas_study})!=0 & ${\"TRUE\" if multi_group else \"FALSE\"}) ${gwas_study} else unique(names(region_data_files))\n",
    "\n",
    "    # assess global parameters\n",
    "    for (study in gwas_studies){\n",
    "        region_files <- region_data_files[grepl(paste0(study, \".chr\"), region_data_files)]\n",
    "        region_data <- setNames(lapply(region_files, readRDS), names(region_files))\n",
    "        contexts <- unique(find_data(region_data, c(4, \"context\")))\n",
    "        # subset either study specific or study-context specific region data based on multigroup/single group\n",
    "        if (${\"TRUE\" if multi_group else \"FALSE\"}) {\n",
    "            ## subset region data files that are not context-specific but study-specific \n",
    "            region_data <- region_data[!sapply(names(region_data), function(x) any(sapply(contexts, function(i) grepl(i, x))))]\n",
    "            group_name <- 'multi'\n",
    "        } else {\n",
    "            region_data <- region_data[sapply(names(region_data), function(x) any(sapply(contexts, function(i) grepl(i, x))))]\n",
    "            group_name <- 'single'      \n",
    "        }\n",
    "        region_data <- lapply(split(region_data, names(region_data)), function(x) do.call(c, unname(x)))\n",
    "        saveRDS(region_data, file.path(outputdir, paste0(\"${name}.region_data_merged.\", group_name,\".\", study,\".thin${thin}.rds\")), compress='xz') #merge and save into one file to be used later\n",
    "        if (length(region_data[[study]])==0) stop(\"Study \", study, \" not present in region_data \", file.path(outputdir, paste0(\"${name}.region_data_merged.\",  group_name, \".thin${thin}.rds\")))\n",
    "        group_prior_file <- file.path(outputdir, paste0(\"${name}.ctwas_param_${prior_var_structure}.\", study, \".thin${thin}.rds\"))\n",
    "        message(\"Start global parameter estimation for \", study, \". \")\n",
    "        param <- est_param(region_data[[study]],\n",
    "                  group_prior_var_structure = \"${prior_var_structure}\", \n",
    "                  niter_prefit = 3,\n",
    "                  niter = ${niter},\n",
    "                  min_group_size = 10,\n",
    "                  ncore = ${numThreads},\n",
    "                  verbose = TRUE)\n",
    "        saveRDS(param, group_prior_file, compress='xz')\n",
    "        region_data[[study]] <- NULL\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "kernel": "SoS",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "[ctwas_3]\n",
    "## ctwas_3: perform finemapping for each region \n",
    "parameter: thin=1.0\n",
    "parameter: maxSNP = 20000\n",
    "parameter: max_iter = 0\n",
    "parameter: run_finemapping = False\n",
    "parameter: prior_var_structure = \"shared_all\"\n",
    "# A list of regions to be subset for screening and fine-mapping, for example: \"10_80126158_82231647\"\n",
    "parameter: region_name =[]\n",
    "parameter: numThreads = 4\n",
    "parameter: multi_group = True\n",
    "parameter: merge_regions=False\n",
    "parameter: L=5\n",
    "import glob\n",
    "\n",
    "skip_if(run_finemapping == False, \" Skip [ctwas_3] fine-mapping. \" )\n",
    "thin=int(thin) if thin == 1.0 else thin\n",
    "name = f\"{name}_{name_suffix}\" if name_suffix else name\n",
    "base = f\"{cwd}/{step_name.split('_')[0]}/{name}\"\n",
    "\n",
    "region_data_file = (\n",
    "    [f\"{base}.region_data_merged.multi.{study}.thin{thin}.rds\" for study in gwas_study]\n",
    "    if multi_group\n",
    "    else [x for x in glob.glob(f\"{cwd:a}/{step_name.split('_')[0]}/{name}.region_data_merged.single.*.thin{thin}.rds\")]\n",
    ")\n",
    "\n",
    "region_info_list = []\n",
    "for region in region_name:\n",
    "    chrom = region.split(\"_\")[0]\n",
    "    if gwas_study:\n",
    "        weights = [f\"{base}.ctwas_weights.{study}.{chrom}.rds\" for study in gwas_study]\n",
    "        zfiles = [f\"{base}.z_gene_snp.{study}.{chrom}.rds\" for study in gwas_study]\n",
    "        params = [x for study in gwas_study for x in glob.glob(f\"{base}.ctwas_param_{prior_var_structure}.{study}*.thin{thin}.rds\")]\n",
    "    else:\n",
    "        weights = [x for x in glob.glob(f\"{base}.ctwas_weights.*.{chrom}.rds\")]\n",
    "        zfiles = [x for x in glob.glob(f\"{base}.z_gene_snp.*.{chrom}.rds\")]\n",
    "        params = [x for x in glob.glob(f\"{base}.ctwas_param_{prior_var_structure}.*.thin{thin}.rds\")]\n",
    "    region_info_list.append({\n",
    "        \"region_name\": region,\n",
    "        \"weights\": weights,\n",
    "        \"zfiles\": zfiles,\n",
    "        \"params\": params\n",
    "    })\n",
    "gwas_study = 'c(' + ', '.join(f'\"{x}\"' for x in gwas_study) + ')'\n",
    "\n",
    "input: region_info_list[_index][\"params\"], for_each = \"region_info_list\"\n",
    "region_name = region_info_list[_index]['region_name']\n",
    "weight_files = region_info_list[_index]['weights']\n",
    "z_snp_file  = region_info_list[_index]['zfiles']\n",
    "params = region_info_list[_index]['params']\n",
    "chrom = region_name.split(\"_\")[0]\n",
    "depends: region_data_file, f\"{cwd}/{step_name.split('_')[0]}/{name}.snp_map.{chrom}.rds\",f\"{cwd}/{step_name.split('_')[0]}/{name}.LD_map.rds\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "R: expand = '${ }', stdout = f\"{cwd:a}/{step_name.split('_')[0]}/{name}.ctwas_finemap_res.{prior_var_structure}.{region_name}.thin{thin}.stdout\", \n",
    "stderr = f\"{cwd:a}/{step_name.split('_')[0]}/{name}.ctwas_finemap_res.{prior_var_structure}.{region_name}.thin{thin}.stderr\", container = container, entrypoint = entrypoint\n",
    "\n",
    "    library(ctwas)\n",
    "    library(pecotmr)\n",
    "    library(data.table)\n",
    "    outputdir = \"${cwd:a}/${step_name.split('_')[0]}\"\n",
    "\n",
    "    gwas_meta_data <- fread(\"${gwas_meta_data}\",data.table=FALSE)\n",
    "    gwas_studies <- if (length(${gwas_study})!=0) ${gwas_study} else unique(gwas_meta_data$study_id)[,1]\n",
    "\n",
    "    weight_files <- ${'c(' + ', '.join(f'\"{x}\"' for x in weight_files) + ')'}\n",
    "    z_snp_files <- ${'c(' + ', '.join(f'\"{x}\"' for x in z_snp_file) + ')'}\n",
    "    param_study_files <- ${'c(' + ', '.join(f'\"{x}\"' for x in params) + ')'}\n",
    "    names(param_study_files) <- gsub('^.*.ctwas_param_${prior_var_structure}.\\\\s*|\\\\s*.thin${thin}.*$', '', param_study_files)\n",
    "    param <- setNames(lapply(unname(param_study_files), readRDS), names(param_study_files))\n",
    "    if (${\"TRUE\" if multi_group else \"FALSE\"}){\n",
    "        param <- param[!sapply(names(param), function(x) any(sapply(paste0(gwas_studies, \".\"), function(c) grepl(c, x))))] # gwas_study per each prior \n",
    "    } else {\n",
    "        param <- param[sapply(names(param), function(x) any(sapply(paste0(gwas_studies, \".\"), function(c) grepl(c, x))))] # gwas_study x context pair per each prior \n",
    "    }\n",
    "\n",
    "    region_data_files <- ${'c(' + ', '.join(f'\"{x}\"' for x in region_data_file) + ')'}\n",
    "    names(weight_files) <- gsub('^.*.ctwas_weights.\\\\s*|\\\\s*.${chrom}.*$', '', weight_files) # gwas study names regardless of single/multigroup\n",
    "    LD_map <- readRDS(\"${cwd}/${step_name.split('_')[0]}/${name}.LD_map.rds\")\n",
    "    snp_map <- readRDS(\"${cwd}/${step_name.split('_')[0]}/${name}.snp_map.${chrom}.rds\")\n",
    "    names(z_snp_files) <- gsub('^.*.z_gene_snp.\\\\s*|\\\\s*.${chrom}.*$', '', z_snp_files)\n",
    "\n",
    "    ## loop through gwas studies (multigroup) / gwas_study_context groups (single_group)\n",
    "    for (study in names(param)){\n",
    "        region_data <- readRDS(region_data_files[grepl(study, region_data_files)])\n",
    "        finemap_res_file <- file.path(outputdir, paste0(\"${name}.ctwas_finemap_res.${prior_var_structure}.${region_name}.\", study, \".thin${thin}.tsv.gz\"))\n",
    "        susie_alpha_file <- gsub(\".tsv.gz\", \".rds\", gsub(\"ctwas_finemap_res\", \"ctwas_susie_alpha_res\", finemap_res_file))\n",
    "        if (nrow(region_data[[study]][[gsub(\"chr\", \"\", \"${region_name}\")]]$z_gene)==0) {\n",
    "            message(\"No z_gene data available for \", study, \" in ${region_name}. \")\n",
    "            fwrite(data.frame(), finemap_res_file, sep = \"\\t\", compress = \"gzip\")\n",
    "            next\n",
    "        }\n",
    "        \n",
    "        gwas_study <- study \n",
    "        if (${\"TRUE\" if multi_group else \"FALSE\"}){\n",
    "            weights <- readRDS(weight_files[gwas_study])\n",
    "        } else { # single group \n",
    "            context <- gsub( \"\\\\|.*$\", \"\", names(param[[study]][['group_prior']])[1])\n",
    "            gwas_study <- gsub(paste0(\".\", context),\"\", study)\n",
    "            weights <- readRDS(weight_files[gwas_study])\n",
    "            if (length(weights[grepl(context, names(weights))])==0) next # no context specific weight data available \n",
    "            weights <- weights[which(grepl(context, names(weights)))] # subset\n",
    "        }\n",
    "        group_prior <- param[[study]]$group_prior\n",
    "        group_prior_var <- param[[study]]$group_prior_var \n",
    "        z_snp_list <- readRDS(z_snp_files[gwas_study])\n",
    "        z_snp <- z_snp_list$z_snp\n",
    "        z_gene <- z_snp_list$z_gene\n",
    "\n",
    "        if (${thin} < 1){\n",
    "            region_data[[study]][gsub(\"chr\", \"\", \"${region_name}\")] <- expand_region_data(region_data[[study]][[gsub(\"chr\", \"\", \"${region_name}\")]],\n",
    "                                              snp_map,\n",
    "                                              z_snp,\n",
    "                                              maxSNP = ${maxSNP},\n",
    "                                              ncore = ${numThreads})\n",
    "        }\n",
    "        screen_res <- screen_regions(region_data[[study]][gsub(\"chr\", \"\", \"${region_name}\")],\n",
    "                                       group_prior = group_prior, group_prior_var = group_prior_var, min_nonSNP_PIP = 0.5, \n",
    "                                       ncore = ${numThreads}, verbose = FALSE, logfile = file.path(outputdir, \n",
    "                                       paste0(\"${name}.screen_regions.${prior_var_structure}.${region_name}.\", study, \".thin${thin}.log\")))\n",
    "        screened_region_data <- screen_res$screened_region_data\n",
    "        # screen_summary <- screen_res$screen_summary\n",
    "        saveRDS(screen_res, file.path(outputdir, paste0(\"${name}.screen_regions.${prior_var_structure}.${region_name}.\", study, \".thin${thin}.rds\")))\n",
    "        if (length(screened_region_data)==0) {\n",
    "            message(\"No region selected for \", study, \" in ${region_name}. \")\n",
    "            fwrite(data.frame(), finemap_res_file, sep = \"\\t\", compress = \"gzip\")\n",
    "            next\n",
    "        }\n",
    "        message(\"Screening region completed for \", study, \". \")\n",
    "\n",
    "        # finemap_regions() argument input \n",
    "        args <- list(screened_region_data, LD_map = LD_map, weights = weights, group_prior = group_prior, \n",
    "                     group_prior_var = group_prior_var, L = ${L}, ncore = ${numThreads}, save_cor = FALSE, LD_format = \"custom\", \n",
    "                     LD_loader_fun = ctwas_ld_loader, snpinfo_loader_fun = ctwas_bimfile_loader, verbose = TRUE, logfile = file.path(outputdir, \n",
    "                     paste0(\"${name}.finemap_res.${prior_var_structure}.${region_name}.\", study, \".thin${thin}.log\")))\n",
    "        if (as.logical(${max_iter})){\n",
    "            finemap_res_file <- gsub(study, paste0(study,\"_max_iter${max_iter}\"),finemap_res_file)\n",
    "            susie_alpha_file <- gsub(study, paste0(study,\"_max_iter${max_iter}\"),susie_alpha_file)\n",
    "            args$max_iter <- ${max_iter}\n",
    "        }\n",
    "        # ctwas finemapping \n",
    "        res <- do.call(finemap_regions, args)\n",
    "        finemap_res <- res$finemap_res\n",
    "        finemap_res$pval <- z2p(finemap_res$z)\n",
    "        susie_alpha_res <- res$susie_alpha_res\n",
    "        fwrite(finemap_res, finemap_res_file, sep = \"\\t\", compress = \"gzip\")\n",
    "        saveRDS(susie_alpha_res, susie_alpha_file, compress='xz')\n",
    "\n",
    "        # Get LD diagnosis \n",
    "        ld_diag <- diagnose_LD_mismatch_susie(region_ids = gsub(\"chr\", \"\", \"${region_name}\"), \n",
    "                                  z_snp = z_snp,\n",
    "                                  LD_map = LD_map, \n",
    "                                  gwas_n = nrow(z_snp),\n",
    "                                  p_diff_thresh = 5e-8,\n",
    "                                  ncore = ${numThreads},\n",
    "                                  LD_format = \"custom\",\n",
    "                                  LD_loader_fun = ctwas_ld_loader,\n",
    "                                  snpinfo_loader_fun = ctwas_bimfile_loader)\n",
    "        problematic_genes <- get_problematic_genes(ld_diag$problematic_snps, \n",
    "                                           weights, \n",
    "                                           finemap_res, \n",
    "                                           pip_thresh = 0.5)\n",
    "        # finemapping with no LD for problematic genes \n",
    "        problematic_region_ids <- unlist(unique(finemap_res[finemap_res$id %in% problematic_genes, \"region_id\"]))\n",
    "        if (length(problematic_region_ids) > 0) {\n",
    "          message(problematic_region_ids)\n",
    "          rerun_region_data <- screened_region_data[problematic_region_ids] # using already expanded screened_region_data\n",
    "          res <- finemap_regions_noLD(rerun_region_data, \n",
    "                                      group_prior = group_prior,\n",
    "                                      group_prior_var = group_prior_var)\n",
    "          finemap_res <- res$finemap_res\n",
    "          susie_alpha_res <- res$susie_alpha_res\n",
    "          message(\"Fine-mapping without LD for region ${region_name} with \", study, \". \")\n",
    "          finemap_res_file <- gsub(study, paste0(study, \"_update\"), finemap_res_file)\n",
    "          susie_alpha_file <- gsub(study, paste0(study, \"_update\"), susie_alpha_file)\n",
    "          fwrite(finemap_res, finemap_res_file, sep = \"\\t\", compress = \"gzip\")\n",
    "          saveRDS(susie_alpha_res, susie_alpha_file, compress='xz')\n",
    "        }\n",
    "        saveRDS(ld_diag[1:3], file.path(outputdir, paste0(\"${name}.ctwas_ld_diag.${prior_var_structure}.${region_name}.\", study,\".thin${thin}.rds\")))\n",
    "        pdf(file.path(outputdir, paste0(\"${name}.ctwas_ld_diag_plot.${prior_var_structure}.${region_name}.\", study,\".thin${thin}.pdf\")), width = 7, height = 7)\n",
    "        print(ld_diag$plots)\n",
    "        dev.off()\n",
    "        message(\"Fine-mapping completed for region ${region_name} with \", study, \". \")\n",
    "        rm(screened_region_data, ld_diag, susie_alpha_res, res, screen_res)\n",
    "        gc()\n",
    "\n",
    "        ## region merging for genes in multiple regions  \n",
    "        boundary_genes <- file.path(outputdir, paste0(\"${name}.ctwas_boundary_genes.\", study, \".${chrom}.thin${thin}.rds\"))\n",
    "        if (file.exists(boundary_genes) & ${\"TRUE\" if merge_regions else \"FALSE\"}){\n",
    "            boundary_genes <- readRDS(boundary_genes)\n",
    "            high_PIP_finemap_gene_res <- subset(finemap_res, group != \"SNP\" & susie_pip > 0.5 & !is.na(cs))\n",
    "            high_PIP_genes <- unique(high_PIP_finemap_gene_res$id)\n",
    "            selected_boundary_genes <- boundary_genes[boundary_genes$id %in% high_PIP_genes, , drop=FALSE]\n",
    "            if (nrow(selected_boundary_genes)!=0){\n",
    "                message(\"Fine-mapping for merged region for ${region_name} with \", study, \". \")\n",
    "                region_info <- get_ctwas_meta_data(\"${ld_meta_data}\", names(snp_map))$region_info\n",
    "                merge_region_res <- merge_region_data(selected_boundary_genes,\n",
    "                                               region_data[[study]],\n",
    "                                               region_info = region_info,\n",
    "                                               LD_map = LD_map,\n",
    "                                               snp_map = snp_map,\n",
    "                                               z_snp = z_snp,\n",
    "                                               z_gene = z_gene,\n",
    "                                               maxSNP = ${maxSNP})\n",
    "                finemap_merged_regions_res <- finemap_regions(merge_region_res$merged_region_data,\n",
    "                                              LD_map = merge_region_res$merged_LD_map,\n",
    "                                              weights = weights,\n",
    "                                              group_prior = group_prior,\n",
    "                                              group_prior_var = group_prior_var,\n",
    "                                              save_cor = FALSE,\n",
    "                                              LD_format = \"custom\", \n",
    "                                              LD_loader_fun = ctwas_ld_loader, \n",
    "                                              snpinfo_loader_fun = ctwas_bimfile_loader)\n",
    "                finemap_res_file <- gsub(study, paste0(study,\"_merged\"),finemap_res_file)\n",
    "                susie_alpha_file <- gsub(study, paste0(study, \"_merged\"), susie_alpha_file)\n",
    "                finemap_res <- finemap_merged_regions_res$finemap_res\n",
    "                finemap_res$pval <- z2p(finemap_res$z)\n",
    "                fwrite(finemap_res, finemap_res_file, sep = \"\\t\", compress = \"gzip\")\n",
    "                saveRDS(finemap_merged_regions_res$susie_alpha_res, susie_alpha_file, compress='xz')\n",
    "            } else {\n",
    "                message(\"No high pip genes found for merged region for fine-mapping. \")\n",
    "            }\n",
    "        }\n",
    "        rm(merge_region_res,boundary_genes,finemap_res, weights, screen_res)\n",
    "        gc()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[quantile_twas]\n",
    "depends: sos_variable(\"filtered_regional_xqtl_files\")\n",
    "parameter: save_ctwas_data = True\n",
    "parameter: save_mr_result = False\n",
    "input: filtered_regional_xqtl_files, group_by = lambda x: group_by_region(x, filtered_regional_xqtl_files), group_with = \"filtered_region_info\"\n",
    "output_files = [f'{cwd:a}/{step_name}/{name}.{_filtered_region_info[3]}.quantile_twas.tsv.gz']\n",
    "if save_ctwas_data:\n",
    "    output_files.append(f'{cwd:a}/{step_name}/{name}.{_filtered_region_info[3]}.quantile_twas_data.rds')\n",
    "if save_mr_result:\n",
    "    output_files.append(f'{cwd:a}/{step_name}/{name}.{_filtered_region_info[3]}.quantile_mr_result.tsv.gz')\n",
    "output: output_files\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]:n}.stdout\", stderr = f\"{_output[0]:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "\n",
    "    library(dplyr)\n",
    "    library(data.table)\n",
    "    library(pecotmr)\n",
    "    library(readr)\n",
    "    \n",
    "    # get xQTL weight information\n",
    "    xqtl_meta_df <- fread(\"${xqtl_meta_data}\") # Get related gene information from the xqtl_meta data table\n",
    "    xqtl_type_table <- if (isTRUE(file.exists(\"${xqtl_type_table}\"))) fread(\"${xqtl_type_table}\") else NULL\n",
    "    gwas_studies = c(${paths(regional_data[\"GWAS\"].keys()):r,})\n",
    "    gwas_files = c(${paths([v[_filtered_region_info[0]] for k, v in regional_data[\"GWAS\"].items()]):r,})\n",
    "    gene_list <- c(${', '.join([f\"'{gene}'\" for gene in _filtered_region_info[4]])})\n",
    "\n",
    "    # Initialize export_twas_weights_db\n",
    "    export_twas_weights_db <- list()\n",
    "    export_twas_weights_db[[\"${_filtered_region_info[3]}\"]] <- list()\n",
    "\n",
    "    # Initialize twas_weights_results\n",
    "    twas_weights_results <- list()\n",
    "\n",
    "    # Create initial weight_db_list\n",
    "    weight_db_list <- c(${_input:r,})\n",
    "    names(weight_db_list) <- gene_list\n",
    "\n",
    "    # Split weight_db_list\n",
    "    weight_db_list <- split(weight_db_list, names(weight_db_list))\n",
    "\n",
    "    # Update weight_db_list\n",
    "    weight_db_list_update <- lapply(weight_db_list, function(file_list) {\n",
    "        valid_files <- Filter(function(file) {\n",
    "            if (file.size(file) > 200) {\n",
    "                content <- tryCatch({\n",
    "                    data <- readRDS(file)\n",
    "                }, error = function(e) {\n",
    "                    warning(paste(\"Failed to read RDS file:\", file))\n",
    "                    NULL\n",
    "                })\n",
    "\n",
    "                return(!is.null(content) && any(sapply(content, function(gene_data) {\n",
    "                    is.list(gene_data) && any(sapply(gene_data, function(context_data) {\n",
    "                        is.list(context_data) && \"twas_variant_names\" %in% names(context_data)\n",
    "                    }))\n",
    "                })))\n",
    "            }\n",
    "            return(FALSE)\n",
    "        }, file_list)\n",
    "\n",
    "        if (length(valid_files) == 0) return(NULL)\n",
    "\n",
    "        return(valid_files)\n",
    "    })\n",
    "\n",
    "    weight_db_list_update <- Filter(Negate(is.null), weight_db_list_update)\n",
    "\n",
    "    if (length(weight_db_list_update) == 0 || all(sapply(weight_db_list_update, length) == 0)) {\n",
    "        message(\"No valid twas weight files found after filtering. Exiting the script.\")\n",
    "        quit(save = \"no\", status = 0) \n",
    "    }\n",
    "\n",
    "    # Check if weight_db_list_update is empty\n",
    "    # Define tau_values\n",
    "    tau_values <- seq(0.01, 0.99, 0.01)\n",
    "    # Main processing loop\n",
    "    for (gene_db in names(weight_db_list_update)) {\n",
    "        weight_dbs <- weight_db_list_update[[gene_db]]\n",
    "        \n",
    "        twas_weights_results[[gene_db]] <- load_quantile_twas_weights(\n",
    "            weight_db_files = weight_dbs,\n",
    "            tau_values = tau_values,\n",
    "            between_cluster = 0.8,\n",
    "            num_intervals = 3\n",
    "        )\n",
    "        \n",
    "        if (!is.null(twas_weights_results[[gene_db]]) && !is.null(twas_weights_results[[gene_db]]$weights)) {\n",
    "            twas_weights_results[[gene_db]]$data_type <- setNames(\n",
    "                lapply(names(twas_weights_results[[gene_db]]$weights), function(context) {\n",
    "                    xqtl_type_table$type[sapply(xqtl_type_table$context, function(x) grepl(x, context))]\n",
    "                }),\n",
    "                names(twas_weights_results[[gene_db]]$weights)\n",
    "            )\n",
    "        } else {\n",
    "            print(paste(\"Warning: No valid weights found for gene:\", gene_db))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if (length(twas_weights_results) == 0 || all(sapply(twas_weights_results, is.null))) {\n",
    "        stop(\"twas_weights_results is empty or invalid. Exiting.\")\n",
    "    }\n",
    "\n",
    "    saveRDS(twas_weights_results, \"${_output[0]:nnn}.grouped_quantile_twas_weight.rds\", compress='xz')\n",
    "    #twas_weights_results\n",
    "    #Step 2: twas analysis for imputable genes across contexts\n",
    "    twas_results_db <- twas_pipeline(twas_weights_data = twas_weights_results, \n",
    "                                    ld_meta_file_path = \"${ld_meta_data}\", \n",
    "                                    gwas_meta_file = \"${gwas_meta_data}\", \n",
    "                                    region_block = \"${_filtered_region_info[3]}\", \n",
    "                                    quantile_twas = TRUE,  \n",
    "                                    output_twas_data = ${\"TRUE\" if save_ctwas_data else \"FALSE\"} )\n",
    "\n",
    "    # Merging with xQTL meta-data      \n",
    "    if (is.null(twas_results_db$twas_result) || nrow(twas_results_db$twas_result) == 0) {\n",
    "        message(\"twas_results_db$twas_result is NULL. Exiting script normally.\")\n",
    "        quit(save = \"no\", status = 0)  \n",
    "    }\n",
    "\n",
    "    message(\"Merging twas_result with xqtl_meta_df...\")    \n",
    "\n",
    "    # Check twas_results_db before merging\n",
    "    common_ids <- intersect(twas_results_db$twas_result$molecular_id, xqtl_meta_df$region_id)       \n",
    "    if (length(common_ids) > 0) {\n",
    "        twas_results_db$twas_result <- merge(\n",
    "            twas_results_db$twas_result,\n",
    "            xqtl_meta_df[, c(\"region_id\", \"TSS\", \"start\", \"end\")],\n",
    "            by.x = \"molecular_id\",\n",
    "            by.y = \"region_id\"\n",
    "        )\n",
    "        twas_results_db$twas_result <- unique(twas_results_db$twas_result)\n",
    "    } else {\n",
    "        warning(\"No common molecular_id and region_id. Skipping merge.\")\n",
    "    }\n",
    "    \n",
    "    \n",
    "    fwrite(twas_results_db$twas_result[, c(2, 1, (ncol(twas_results_db$twas_result)-2):ncol(twas_results_db$twas_result), 3:(ncol(twas_results_db$twas_result)-3))], file = ${_output[0]:r}, sep = \"\\t\", compress = \"gzip\")\n",
    "\n",
    "    # Step 3: reformat for follow up cTWAS analysis\n",
    "    if (${\"TRUE\" if save_ctwas_data else \"FALSE\"}) {\n",
    "        saveRDS(twas_results_db$twas_data, \"${_output[0]:nnn}.quantile_twas_data.rds\", compress='xz')\n",
    "    }\n",
    "    if (${\"TRUE\" if save_mr_result else \"FALSE\"}) {\n",
    "        fwrite(twas_results_db$mr_result, file = \"${_output[0]:nnn}.quantile_mr_result.tsv.gz\", sep = \"\\t\", compress = \"gzip\")\n",
    "    }\n",
    "    message(\"quantile twas analysis is completed in this block.\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
