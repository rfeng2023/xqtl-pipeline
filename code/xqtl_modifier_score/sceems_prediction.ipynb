{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "# EMS Prediction\n",
    "\n",
    "**Author**: Angelina Siagailo(angiesiagailo@gmail.com) with input from Gao Wang and Katie Cho. \n",
    "\n",
    "**Apply the trained sc-EMS model to score YOUR genetic variants for functional impact.** This notebook demonstrates prediction workflows using the model trained in the [EMS Training Tutorial](https://statfungen.github.io/xqtl-protocol/code/xqtl_modifier_score/ems_training.html).\n",
    "\n",
    "## What This Does\n",
    "- **Input**: Your variant list in VCF-style format (`chr:pos:ref:alt`)\n",
    "- **Output**: Variants annotated with EMS functional probability scores (continuous range: 0-1)\n",
    "  - **Score >0.8**: High functional probability - prioritize for experimental validation (CRISPR screens, luciferase assays)\n",
    "  - **Score 0.5-0.8**: Moderate functional probability - context-dependent prioritization\n",
    "  - **Score <0.5**: Low functional probability\n",
    "- **Purpose**: Prioritize variants likely to affect gene expression using the trained feature-weighted CatBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4-f5a6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "## Pipeline Execution Workflow\n",
    "\n",
    "### Step 1: Input Data Preparation\n",
    "\n",
    "Create a tab-separated file with variant identifiers:\n",
    "```\n",
    "variant_id\n",
    "2:12345:A:T\n",
    "2:67890:G:C\n",
    "2:11111:T:A\n",
    "```\n",
    "\n",
    "**Format requirements:**\n",
    "- Chromosome notation: Numeric (e.g., `2`) or with prefix (e.g., `chr2`)\n",
    "- Position: 1-based genomic coordinate (GRCh38/hg38 reference)\n",
    "- Alleles: Reference and alternate alleles (ACGT notation)\n",
    "\n",
    "### Step 2: Execute Prediction Pipeline\n",
    "\n",
    "```bash\n",
    "cd ~/xqtl-protocol/code/xqtl_modifier_score/\n",
    "python model_training_model5_only.py Mic_mega_eQTL 2 \\\n",
    "  --data_config data_config.yaml \\\n",
    "  --model_config model_config.yaml\n",
    "```\n",
    "\n",
    "**What happens during execution:**\n",
    "1. Variant parsing and coordinate validation\n",
    "2. Feature annotation (distance, regulatory, population genetics, conservation, deep learning predictions)\n",
    "3. Gene constraint and MAF integration with imputation using training statistics\n",
    "4. Feature subsetting and absolute value transformations\n",
    "5. Model inference with 10x weighting for regulatory features\n",
    "6. Probability calibration and output generation\n",
    "\n",
    "### Step 3: Output Files Generated\n",
    "\n",
    "| File | Description | Use Case |\n",
    "|------|-------------|----------|\n",
    "| `model_standard_subset_weighted_chr_chr2_NPR_10.joblib` | Serialized CatBoost model | Future predictions, model inspection |\n",
    "| `predictions_weighted_model_chr2.tsv` | Per-variant predictions with scores | Primary analysis, variant prioritization |\n",
    "| `summary_dict_catboost_weighted_model_chr_chr2_NPR_10.pkl` | AP/AUC metrics, class distributions | Model validation, quality control |\n",
    "| `features_importance_model5_chr_chr2_NPR_10.csv` | Feature importance rankings | Biological interpretation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4-f5a6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "## Load and Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2g3h4-i5j6-7890-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load trained CatBoost model\n",
    "MODEL_PATH = \"../../data/Mic_mega_eQTL/model_results/model_standard_subset_weighted_chr_chr2_NPR_10.joblib\"\n",
    "model = joblib.load(MODEL_PATH)\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Algorithm: {model.__class__.__name__}\")\n",
    "print(f\"  Features: {model.feature_count_}\")\n",
    "print(f\"  Classes: {list(model.classes_)}\")\n",
    "print(f\"  Tree depth: {model.get_params()['depth']}\")\n",
    "print(f\"  Iterations: {model.tree_count_}\")\n",
    "\n",
    "# Load prediction results\n",
    "RESULTS_PATH = \"../../data/Mic_mega_eQTL/model_results/predictions_parquet_catboost/predictions_weighted_model_chr2.tsv\"\n",
    "results = pd.read_csv(RESULTS_PATH, sep='\\t')\n",
    "\n",
    "print(f\"\\nLoaded predictions for {len(results)} variants\")\n",
    "print(f\"\\nAvailable columns:\")\n",
    "print(f\"  - variant_id: Genomic coordinates\")\n",
    "print(f\"  - standard_subset_weighted_pred_prob: EMS functional score (0-1)\")\n",
    "print(f\"  - standard_subset_weighted_pred_label: Binary classification (0/1)\")\n",
    "print(f\"  - actual_label: True label from test set (if available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example_output_1",
   "metadata": {},
   "source": [
    "**Example output:**\n",
    "```\n",
    "Model Configuration:\n",
    "  Algorithm: CatBoostClassifier\n",
    "  Features: 4839\n",
    "  Classes: [0, 1]\n",
    "  Tree depth: 6\n",
    "  Iterations: 1000\n",
    "\n",
    "Loaded predictions for 761 variants\n",
    "\n",
    "Available columns:\n",
    "  - variant_id: Genomic coordinates\n",
    "  - standard_subset_weighted_pred_prob: EMS functional score (0-1)\n",
    "  - standard_subset_weighted_pred_label: Binary classification (0/1)\n",
    "  - actual_label: True label from test set (if available)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f3g4-f5a6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "## Statistical Summary of Predictions\n",
    "\n",
    "The `standard_subset_weighted_pred_prob` column contains continuous EMS scores representing the probability that each variant has functional regulatory impact on gene expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1g2h3i4-j5k6-7890-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_col = 'standard_subset_weighted_pred_prob'\n",
    "\n",
    "# Quantitative distribution analysis\n",
    "print(\"SCORE DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total variants analyzed: {len(results)}\")\n",
    "print(f\"\\nPriority Classification:\")\n",
    "high = len(results[results[score_col] > 0.8])\n",
    "medium = len(results[(results[score_col] > 0.5) & (results[score_col] <= 0.8)])\n",
    "low = len(results[results[score_col] <= 0.5])\n",
    "print(f\"  High priority (>0.8):     {high:5d} ({high/len(results)*100:5.1f}%)\")\n",
    "print(f\"  Medium priority (0.5-0.8): {medium:5d} ({medium/len(results)*100:5.1f}%)\")\n",
    "print(f\"  Low priority (<0.5):      {low:5d} ({low/len(results)*100:5.1f}%)\")\n",
    "\n",
    "# Descriptive statistics\n",
    "print(f\"\\nScore Statistics:\")\n",
    "print(f\"  Mean:   {results[score_col].mean():.4f}\")\n",
    "print(f\"  Median: {results[score_col].median():.4f}\")\n",
    "print(f\"  Std:    {results[score_col].std():.4f}\")\n",
    "print(f\"  Min:    {results[score_col].min():.4f}\")\n",
    "print(f\"  Max:    {results[score_col].max():.4f}\")\n",
    "\n",
    "# Percentile distribution\n",
    "print(f\"\\nPercentile Distribution:\")\n",
    "for p in [90, 95, 99]:\n",
    "    val = np.percentile(results[score_col], p)\n",
    "    count = len(results[results[score_col] >= val])\n",
    "    print(f\"  {p}th percentile: {val:.4f} ({count} variants)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example_output_2",
   "metadata": {},
   "source": [
    "**Example output:**\n",
    "```\n",
    "SCORE DISTRIBUTION ANALYSIS\n",
    "==================================================\n",
    "Total variants analyzed: 761\n",
    "\n",
    "Priority Classification:\n",
    "  High priority (>0.8):        12 (  1.6%)\n",
    "  Medium priority (0.5-0.8):   45 (  5.9%)\n",
    "  Low priority (<0.5):        704 ( 92.5%)\n",
    "\n",
    "Score Statistics:\n",
    "  Mean:   0.1245\n",
    "  Median: 0.0823\n",
    "  Std:    0.1567\n",
    "  Min:    0.0012\n",
    "  Max:    0.9234\n",
    "\n",
    "Percentile Distribution:\n",
    "  90th percentile: 0.3421 (76 variants)\n",
    "  95th percentile: 0.5012 (38 variants)\n",
    "  99th percentile: 0.7834 (8 variants)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g1h2i3j4-k5l6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "## Variant Prioritization and Export\n",
    "\n",
    "Extract and rank high-confidence functional predictions for downstream experimental validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h1i2j3k4-l5m6-7890-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and sort high-priority variants\n",
    "high_priority = results[results[score_col] > 0.8].sort_values(score_col, ascending=False)\n",
    "\n",
    "print(f\"HIGH-PRIORITY VARIANTS (Score >0.8)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total: {len(high_priority)} variants\")\n",
    "\n",
    "if len(high_priority) > 0:\n",
    "    print(f\"\\nTop 10 by EMS score:\")\n",
    "    display_cols = ['variant_id', score_col]\n",
    "    if 'actual_label' in results.columns:\n",
    "        display_cols.append('actual_label')\n",
    "    print(high_priority[display_cols].head(10).to_string(index=False))\n",
    "    \n",
    "    # Export for experimental design\n",
    "    output_file = \"high_priority_variants_validation.tsv\"\n",
    "    high_priority.to_csv(output_file, sep='\\t', index=False)\n",
    "    print(f\"\\nExported to: {output_file}\")\n",
    "    print(f\"   Contains all {len(high_priority)} high-priority variants\")\n",
    "    print(f\"   Ready for: CRISPR screens, luciferase assays, functional validation\")\n",
    "else:\n",
    "    print(\"\\nNo variants exceed 0.8 threshold\")\n",
    "    print(\"   Recommended actions:\")\n",
    "    print(\"   1. Review medium-priority variants (0.5-0.8)\")\n",
    "    print(\"   2. Consider lowering threshold based on experimental capacity\")\n",
    "    print(f\"   3. Top variant score: {results[score_col].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example_output_3",
   "metadata": {},
   "source": [
    "**Example output:**\n",
    "```\n",
    "HIGH-PRIORITY VARIANTS (Score >0.8)\n",
    "==================================================\n",
    "Total: 12 variants\n",
    "\n",
    "Top 10 by EMS score:\n",
    "      variant_id  standard_subset_weighted_pred_prob  actual_label\n",
    "   2:54321:A:G                              0.9234             1\n",
    "   2:12345:T:C                              0.9102             1\n",
    "   2:98765:C:A                              0.8956             1\n",
    "   2:44444:G:T                              0.8723             0\n",
    "   2:77777:A:C                              0.8612             1\n",
    "   2:33333:T:G                              0.8501             1\n",
    "   2:66666:C:T                              0.8398             1\n",
    "   2:11111:G:A                              0.8267             0\n",
    "   2:55555:A:T                              0.8145             1\n",
    "   2:99999:T:A                              0.8034             1\n",
    "\n",
    "  Exported to: high_priority_variants_validation.tsv\n",
    "   Contains all 12 high-priority variants\n",
    "   Ready for: CRISPR screens, luciferase assays, functional validation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1j2k3l4-m5n6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "## Verify Model Performance\n",
    "\n",
    "Review metrics from the held-out test set to understand model reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j1k2l3m4-n5o6-7890-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display performance metrics\n",
    "SUMMARY_PATH = \"../../data/Mic_mega_eQTL/model_results/summary_dict_catboost_weighted_model_chr_chr2_NPR_10.pkl\"\n",
    "with open(SUMMARY_PATH, 'rb') as f:\n",
    "    summary = pickle.load(f)\n",
    "\n",
    "print(\"MODEL PERFORMANCE ON TEST SET\")\n",
    "print(\"=\" * 50)\n",
    "metrics = summary['CatBoost']['standard_subset_weighted']\n",
    "print(f\"Average Precision (AP): {metrics['AP_test']:.4f}\")\n",
    "print(f\"AUC-ROC:                {metrics['AUC_test']:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Set Composition:\")\n",
    "print(f\"  Positive labels (functional eQTLs): {summary['CatBoost']['test_num_positive_labels']}\")\n",
    "print(f\"  Negative labels (non-functional):   {summary['CatBoost']['test_num_negative_labels']}\")\n",
    "pos_rate = summary['CatBoost']['test_num_positive_labels'] / (summary['CatBoost']['test_num_positive_labels'] + summary['CatBoost']['test_num_negative_labels'])\n",
    "print(f\"  Positive rate:                       {pos_rate:.1%}\")\n",
    "\n",
    "print(\"\\n These metrics reflect performance on 761 held-out chromosome 2 variants\")\n",
    "print(\"   with stricter selection criteria (PIP >0.9 for positives) than training data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example_output_4",
   "metadata": {},
   "source": [
    "**Example output:**\n",
    "```\n",
    "MODEL PERFORMANCE ON TEST SET\n",
    "==================================================\n",
    "Average Precision (AP): 0.5050\n",
    "AUC-ROC:                0.8978\n",
    "\n",
    "Test Set Composition:\n",
    "  Positive labels (functional eQTLs): 68\n",
    "  Negative labels (non-functional):   693\n",
    "  Positive rate:                       8.9%\n",
    "\n",
    "   These metrics reflect performance on 761 held-out chromosome 2 variants\n",
    "   with stricter selection criteria (PIP >0.9 for positives) than training data.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4-m5n6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "## Application to New Variant Lists\n",
    "\n",
    "### Workflow for Novel Variants\n",
    "\n",
    "To score additional variants not included in the original training/test sets:\n",
    "\n",
    "**1. Prepare input file** matching the format above (`chr:pos:ref:alt`)\n",
    "\n",
    "**2. Update configuration** (`data_config.yaml`):\n",
    "- Point `training_data.base_dir` to directory containing your variant annotations\n",
    "- Ensure gene constraint file (GeneBayes scores) is accessible\n",
    "- Verify MAF file matches your chromosome\n",
    "\n",
    "**3. Execute pipeline** (same command, different input data):\n",
    "```bash\n",
    "python model_training_model5_only.py [cohort] [chromosome] \\\n",
    "  --data_config data_config.yaml \\\n",
    "  --model_config model_config.yaml\n",
    "```\n",
    "\n",
    "The pipeline will:\n",
    "- Generate all 4,839 genomic features for your variants\n",
    "- Apply the same preprocessing (subsetting, absolute values, imputation)\n",
    "- Use the trained model for inference\n",
    "- Output predictions in identical format\n",
    "\n",
    "**4. Analyze results** using the code blocks above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


